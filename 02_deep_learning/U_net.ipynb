{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time as time\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torchnet as tnt\n",
    "import functools\n",
    "\n",
    "import mock\n",
    "from tqdm import notebook as tqdm\n",
    "\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 12                                     # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize]  \n",
    "CMAP = matplotlib.colors.ListedColormap(['black', 'white', 'orange'])               # Color mapping \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision\n",
    "\n",
    "# CLASS AND FEATURE DESCRIPTION\n",
    "class_names = ['BACKGRD','PINUS','PICEA']\n",
    "\n",
    "# PATHS TO TRAIN/TEST DATA\n",
    "data_path = '../data/jeseniky/blackandwhite/'\n",
    "training_set_path = data_path + 'train/'         # Relative path to training patch root folder\n",
    "test_set_path =     data_path + 'test/'          # Relative path to test patch root folder\n",
    "\n",
    "num_of_training_tiles = len(os.listdir(training_set_path + 'GT/'))\n",
    "num_of_test_tiles = len(os.listdir(test_set_path + 'GT/'))\n",
    "\n",
    "# USE CIR OR RGB DATA\n",
    "use_cir = False\n",
    "use_rgb = False\n",
    "use_pan = True\n",
    "\n",
    "# MODEL NAME... USED AS FILENAME OF SAVED MODEL AND FOR APPROPRIATE RESULTS FOLDER\n",
    "model_name = 'U_Net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patch(root_folder, cir, rgb, pan, gt=True):\n",
    "    ##########################################################\n",
    "    # READ IMAGES as FLOAT\n",
    "    \n",
    "    if cir:\n",
    "        cir_file_list = os.listdir(root_folder + 'CIR/')\n",
    "        cir_list = []\n",
    "        \n",
    "        for file in cir_file_list:\n",
    "            cir_patch = imageio.imread(root_folder + 'CIR/' + file).astype(np.float32)\n",
    "            cir_patch = cir_patch[:,:,:].transpose([2,0,1])\n",
    "            cir_patch = cir_patch * 1/255\n",
    "            \n",
    "            cir_list.append(cir_patch)\n",
    "            del cir_patch\n",
    "\n",
    "        cir_features = np.stack(cir_list, axis=0)    \n",
    "    \n",
    "    if rgb:\n",
    "        rgb_file_list = os.listdir(root_folder + 'RGB/')\n",
    "        rgb_list = []\n",
    "        \n",
    "        for file in rgb_file_list:\n",
    "            rgb_patch = imageio.imread(root_folder + 'RGB/' + file).astype(np.float32)\n",
    "            rgb_patch = rgb_patch[:,:,:].transpose([2,0,1])\n",
    "            rgb_patch = rgb_patch * 1/255\n",
    "            \n",
    "            rgb_list.append(rgb_patch)\n",
    "            \n",
    "            del rgb_patch\n",
    "        \n",
    "        rgb_features = np.stack(rgb_list, axis=0)\n",
    "\n",
    "    if pan:\n",
    "        pan_file_list = os.listdir(root_folder + 'PAN/')\n",
    "        pan_list = []\n",
    "        \n",
    "        for file in pan_file_list:\n",
    "            pan_patch = imageio.imread(root_folder + 'PAN/' + file).astype(np.float32)\n",
    "            pan_patch = pan_patch * 1/255\n",
    "            pan_patch = np.expand_dims(pan_patch, axis=0)\n",
    "            \n",
    "            pan_list.append(pan_patch)\n",
    "            \n",
    "            del pan_patch\n",
    "        \n",
    "        pan_features = np.stack(pan_list, axis=0)\n",
    "\n",
    "        \n",
    "    if cir and rgb:\n",
    "        features = np.concatenate([cir_features, rgb_features], axis=1)\n",
    "    elif cir:\n",
    "        features = cir_features\n",
    "    elif rgb:\n",
    "        features = rgb_features\n",
    "    elif pan:\n",
    "        features = pan_features\n",
    "    else:\n",
    "        print('No valid data input.')\n",
    "    features = torch.from_numpy(features)\n",
    "    \n",
    "    \n",
    "    if gt:\n",
    "        gt_file_list = os.listdir(root_folder + 'GT/')\n",
    "        gt_list = []\n",
    "\n",
    "        for file in gt_file_list:\n",
    "            gt_patch = imageio.imread(root_folder + 'GT/' + file).astype(np.int64)\n",
    " \n",
    "            gt_list.append(gt_patch[:,:])\n",
    "            del gt_patch\n",
    "\n",
    "        ground_truth = np.stack(gt_list, axis=0)\n",
    "        ground_truth = torch.from_numpy(ground_truth)\n",
    "    \n",
    "    if gt:\n",
    "        return features, ground_truth\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### putting the dataset into the TensorDataset wrapper\n",
    "X, y = read_patch(training_set_path, use_cir, use_rgb, use_pan)\n",
    "X_t, y_t = read_patch(test_set_path, use_cir, use_rgb, use_pan)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_t.shape)\n",
    "\n",
    "\n",
    "train_set = tnt.dataset.TensorDataset(list([X, y]))\n",
    "test_set  = tnt.dataset.TensorDataset(list([X_t, y_t]))\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net network for semantic segmentation\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        n_channels, int, number of input channel\n",
    "        encoder_conv_width, int list, size of the feature maps of convs for the encoder\n",
    "        decoder_conv_width, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__() #necessary for all classes extending the module class\n",
    "    \n",
    "        self.maxpool=nn.MaxPool2d(2,2,return_indices=False) #maxpooling layer\n",
    "    \n",
    "        #encoder\n",
    "        self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c5 = nn.Sequential(nn.Conv2d(encoder_conv_width[3],encoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c6 = nn.Sequential(nn.Conv2d(encoder_conv_width[4],encoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c7 = nn.Sequential(nn.Conv2d(encoder_conv_width[5],encoder_conv_width[6],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c8 = nn.Sequential(nn.Conv2d(encoder_conv_width[6],encoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c9 = nn.Sequential(nn.Conv2d(encoder_conv_width[7],encoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c10 = nn.Sequential(nn.Conv2d(encoder_conv_width[8],encoder_conv_width[9],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        #decoder\n",
    "        self.c11 = nn.ConvTranspose2d(encoder_conv_width[9], int(decoder_conv_width[0]/2),kernel_size=2, stride=2)\n",
    "        self.c12 = nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c13 = nn.Sequential(nn.Conv2d(decoder_conv_width[1],decoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c14 = nn.ConvTranspose2d(decoder_conv_width[2], int(decoder_conv_width[3]/2),kernel_size=2, stride=2)\n",
    "        self.c15 = nn.Sequential(nn.Conv2d(decoder_conv_width[3],decoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c16 = nn.Sequential(nn.Conv2d(decoder_conv_width[4],decoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c17 = nn.ConvTranspose2d(decoder_conv_width[5], int(decoder_conv_width[6]/2),kernel_size=2, stride=2)\n",
    "        self.c18 = nn.Sequential(nn.Conv2d(decoder_conv_width[6],decoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c19 = nn.Sequential(nn.Conv2d(decoder_conv_width[7],decoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c20 = nn.ConvTranspose2d(decoder_conv_width[8], int(decoder_conv_width[9]/2),kernel_size=2, stride=2)\n",
    "        self.c21 = nn.Sequential(nn.Conv2d(decoder_conv_width[9],decoder_conv_width[10],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c22 = nn.Sequential(nn.Conv2d(decoder_conv_width[10],decoder_conv_width[11],3,padding=1, padding_mode='reflect'),nn.ReLU(True)) \n",
    "        \n",
    "        #final classifying layer\n",
    "        self.classifier=nn.Conv2d(decoder_conv_width[11],n_class,1,padding=0)\n",
    "\n",
    "        #weight initialization\n",
    "\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c12[0].apply(self.init_weights)\n",
    "        self.c13[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c15[0].apply(self.init_weights)\n",
    "        self.c16[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c18[0].apply(self.init_weights)\n",
    "        self.c19[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c21[0].apply(self.init_weights)\n",
    "        self.c22[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "    \n",
    "        if cuda: #put the model on the GPU memory\n",
    "            self.cuda()\n",
    "    \n",
    "    def init_weights(self,layer): #gaussian init for the conv layers\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        the function called to run inference\n",
    "        \"\"\"  \n",
    "        #encoder\n",
    "        #level 1\n",
    "        x1 = self.c2(self.c1(input))\n",
    "        x2 = self.maxpool(x1)\n",
    "        #level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        x4 = self.maxpool(x3)\n",
    "        #level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        x6 = self.maxpool(x5)\n",
    "        #Level 4\n",
    "        x7 = self.c8(self.c7(x6))\n",
    "        x8 = self.maxpool(x7)\n",
    "        #Level 5\n",
    "        x9 = self.c10(self.c9(x8))\n",
    "        #decoder\n",
    "        #Level 4\n",
    "        y8 = torch.cat((self.c11(x9),x7),1)\n",
    "        y7 = self.c13(self.c12(y8))\n",
    "        #Level 3\n",
    "        y6 = torch.cat((self.c14(y7),x5),1)\n",
    "        y5 = self.c16(self.c15(y6))\n",
    "        #level 2\n",
    "        y4 = torch.cat((self.c17(y5),x3),1)\n",
    "        y3 = self.c19(self.c18(y4))\n",
    "        #level 1       \n",
    "        y2 = torch.cat((self.c20(y3),x1),1)\n",
    "        y1 = self.c22(self.c21(y2))\n",
    "        #output         \n",
    "        out = self.classifier(y1)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(obs, g_t):\n",
    "    \"\"\"the augmentation function\n",
    "    do not change until you reach Q14\n",
    "    \"\"\"\n",
    "    sigma, clip= 0.01, 0.03 \n",
    "    #Hint: use np.clip to clip and np.random.randn to generate gaussian noise\n",
    "    obs = obs + np.clip(sigma*np.random.randn(), -clip, clip).astype(np.float32).copy()\n",
    "\n",
    "    #random rotation 0 90 180 270 degree\n",
    "    n_turn = np.random.randint(4) #number of 90 degree truens, random int between 0 and 3\n",
    "    obs = np.rot90(obs, n_turn, axes=(2,3)).copy()\n",
    "    g_t = np.rot90(g_t, n_turn, axes=(1,2)).copy()\n",
    "\n",
    "    obs = torch.from_numpy(obs)\n",
    "    g_t = torch.from_numpy(g_t)\n",
    "    \n",
    "    return obs, g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "        \n",
    "        tiles, gt = augment(tiles, gt)\n",
    "    \n",
    "        pred = model(tiles.cuda()) #compute the prediction\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=torch.tensor(args.class_weights))\n",
    "\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "    \n",
    "        optimizer.step() #one SGD step\n",
    "    \n",
    "        loss_meter.add(loss.item())\n",
    "        \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, args):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda())\n",
    "            loss = nn.functional.cross_entropy(pred.cpu(),gt)\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = UNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class, args.cuda)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "    #define the optimizer\n",
    "    #adam optimizer is always a good guess for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[40,60], gamma=0.1)\n",
    "  \n",
    "    TESTCOLOR = '\\033[104m'\n",
    "    NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "    train_loss = np.empty(args.n_epoch)\n",
    "    test_loss = np.empty(args.n_epoch//args.n_epoch_test)\n",
    "    test_i = 0\n",
    "\n",
    "    for i_epoch in range(args.n_epoch):\n",
    "        #train one epoch\n",
    "        print('Epoch ' + str(i_epoch))\n",
    "        loss_train = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "        train_loss[i_epoch] = loss_train\n",
    "\n",
    "        if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "            #periodic testing\n",
    "            print(TESTCOLOR)\n",
    "            print('Evaluation')\n",
    "            loss_test = eval(model, args)\n",
    "            test_loss[test_i] = loss_test\n",
    "            test_i += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1,1,1,ylim=(0,2), xlabel='Epoch #', ylabel='Loss')\n",
    "    plt.plot(range(args.n_epoch), train_loss)\n",
    "    plt.plot(range(args.n_epoch_test-1, args.n_epoch, args.n_epoch_test), test_loss)\n",
    "    plt.show()\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = mock.Mock() #stores the parameters\n",
    "args.n_epoch = 100\n",
    "args.n_epoch_test = int(5) #periodicity of evaluation on test set\n",
    "args.batch_size = 2\n",
    "args.n_class = len(class_names)\n",
    "args.n_channel = 1 # 6 if use_cir and use_rgb else 3\n",
    "args.conv_width = [64,64,128,128,256,256,512,512,1024,1024]\n",
    "args.dconv_width = [1024,512,512,512,256,256,256,128,128,128,64,64]\n",
    "args.class_weights = [0.2, 0.2, 0.6]\n",
    "args.cuda = True\n",
    "args.lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = time()\n",
    "trained_model = train_full(args)\n",
    "b = time()\n",
    "\n",
    "print('Training finished in ' + str(b-a) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training multiple models\n",
    "learning_rates = [1e-4, 1e-3]\n",
    "trained_models = []\n",
    "for i in learning_rates:\n",
    "    args.lr = i\n",
    "    print('Learning rate for this run is ' + str(i))\n",
    "    a = time()\n",
    "    trained_models.append(train_full(args))\n",
    "    b = time()\n",
    "    print('Training finished in ' + str(b-a) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb_cir_gt_pred(tile_index, data, gt, model, cir, rgb):\n",
    "    # Function to plot prediction vs ground truth\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(facecolor='white')\n",
    "\n",
    "    data = data[tile_index,:,:,:]\n",
    "    pred = model(data[None,:,:,:].cuda()).cpu().detach().numpy()\n",
    "    pred = pred[0,:,:,:].argmax(0).squeeze()\n",
    "    \n",
    "    unique, counts = np.unique(pred, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    \n",
    "    data = data.cpu().numpy()\n",
    "    \n",
    "    if cir and rgb:\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(data[:3].transpose([1,2,0]))\n",
    "        plt.title('NIR Red Green composite')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(data[-3:].transpose([1,2,0]))\n",
    "        plt.title('Red Green Blue composite')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(gt[tile_index,:,:], CMAP)\n",
    "        plt.title('GT Labels')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(pred, CMAP)\n",
    "        plt.title('Predicted Labels')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    elif cir or rgb:\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(data.transpose([1,2,0]))\n",
    "        if cir:\n",
    "            plt.title('NIR Red Green composite')\n",
    "        else:\n",
    "            plt.title('Red Green Blue composite')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gt[tile_index,:,:], CMAP)\n",
    "        plt.title('GT Labels')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred, CMAP)\n",
    "        plt.title('Predicted Labels')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rgb_cir_gt_pred(750, X_t, y_t, trained_model, use_cir, use_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, args):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "    loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "    \n",
    "    classified = np.empty_like(y_t.detach().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda()).cpu().detach().numpy()\n",
    "            classified[index, :, :] = pred.squeeze().argmax(0)\n",
    "\n",
    "    return classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time()\n",
    "Y_t = classify(trained_model, args)\n",
    "b = time()\n",
    "print('Inferrence finished in ' + str(b-a) + ' s')\n",
    "\n",
    "Y_t_flat = Y_t.flatten()\n",
    "\n",
    "unique, counts = np.unique(Y_t_flat, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results for all test tiles into one file, used later for visualisation\n",
    "imageio.imwrite(data_path + 'results/U_Net.tif', Y_t_flat.reshape(2,int(Y_t_flat.shape[0]/2)).astype(np.uint8), bigtiff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_t_flat = y_t.detach().numpy().flatten()\n",
    "\n",
    "unique, counts = np.unique(y_t_flat, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute accuracy metrics\n",
    "precisions, recalls, f1_scores, supports = precision_recall_fscore_support(y_t_flat, Y_t_flat)\n",
    "overall_accuracy = accuracy_score(y_t_flat, Y_t_flat)\n",
    "mean_f1_score = sum(f1_scores)/len(f1_scores)\n",
    "\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('f1_scores  [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean f1 score:    {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and reusing a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the state_dictionary\n",
    "state_dict_path = 'trained_models/U_Net_1989_1e-4.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a model to state_dict_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a trained model state_dictionary\n",
    "torch.save(trained_models[0].state_dict(), state_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse a model at state_dict_path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model definition\n",
    "args = mock.Mock() #stores the parameters\n",
    "\n",
    "args.n_class = len(class_names)\n",
    "args.n_channel = 1 # 6 if use_cir and use_rgb else 3\n",
    "args.conv_width = [64,64,128,128,256,256,512,512,1024,1024]\n",
    "args.dconv_width = [1024,512,512,512,256,256,256,128,128,128,64,64]\n",
    "args.cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model state_dictionary\n",
    "model = UNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class, args.cuda)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb_cir_gt_pred(8, X_t, y_t, model, use_cir, use_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results\n",
    "Results are not georeferenced â€“ use ArcPy_georeference_results.py for georeferencing and combining into a single raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path =  '../data/jeseniky/blackandwhite/'\n",
    "results_path = source_path + 'results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = read_patch(source_path, use_cir, use_rgb, use_pan, gt=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(in_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_rgb:\n",
    "    copy_tree(source_path + 'RGB/', results_path, update=1)\n",
    "elif use_cir:\n",
    "    copy_tree(source_path + 'CIR/', results_path, update=1)\n",
    "elif use_pan:\n",
    "    copy_tree(source_path + 'PAN/', results_path, update=1)\n",
    "else:\n",
    "    print('no input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_export(model_b, in_features_b, results_path_b):\n",
    "    i = 0\n",
    "    for patch in os.listdir(results_path_b):\n",
    "        in_patch = in_features_b[i,:,:,:]\n",
    "        pred = model_b(in_patch[None,:,:,:].cuda()).cpu().detach().numpy()\n",
    "        pred = pred[0,:,:,:].argmax(0).squeeze()\n",
    "\n",
    "        imageio.imwrite(results_path_b + patch, pred.astype(np.uint8))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time()\n",
    "classify_and_export(model, in_features, results_path)\n",
    "b = time()\n",
    "\n",
    "print('Classification finished in ' + str(b-a) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export in bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = '../data/2012/eastern/overlap/'\n",
    "source_list = os.listdir(source_path)\n",
    "results_path = '../data/2012/eastern/results/overlap/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir('../data/2012/western/overlap/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = time()\n",
    "for source_name in source_list:\n",
    "    in_features = read_patch(source_path + source_name + '/', use_cir, use_rgb, gt=False)\n",
    "    \n",
    "    if use_rgb:\n",
    "        copy_tree(source_path + source_name + '/RGB/', results_path + source_name, update=1)\n",
    "    elif use_cir:\n",
    "        copy_tree(source_path + source_name + '/CIR/', results_path + source_name, update=1)\n",
    "    else:\n",
    "        print('no input files')\n",
    "    \n",
    "    classify_and_export(model, in_features, results_path + source_name + '/')\n",
    "    del in_features\n",
    "    print('Finished classifing of ' + results_path + source_name)\n",
    "b = time()\n",
    "print('This took ' + str(b-a) + ' s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
