{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time as time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torchnet as tnt\n",
    "import functools\n",
    "\n",
    "import mock\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 8                                     # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize]  \n",
    "CMAP = matplotlib.colors.ListedColormap(['black', 'white', 'orange'])               # Color mapping \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision\n",
    "\n",
    "# CLASS AND FEATURE DESCRIPTION\n",
    "class_names = ['BACKGRD','PINUS','PICEA']\n",
    "\n",
    "# PATHS TO TRAIN/TEST DATA\n",
    "data_path = '../data/split_05__05/'\n",
    "training_set_path = data_path + 'train/'         # Relative path to training patch root folder\n",
    "test_set_path =     data_path + 'test/'          # Relative path to test patch root folder\n",
    "\n",
    "num_of_training_tiles = len(os.listdir(training_set_path + 'GT/'))\n",
    "num_of_test_tiles = len(os.listdir(test_set_path + 'GT/'))\n",
    "\n",
    "# USE CIR OR RGB DATA\n",
    "use_cir = True\n",
    "use_rgb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "print(torch.cuda.get_device_name())\n",
    "print(torch.cuda.get_device_capability())\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patch(root_folder, cir=True, rgb=True):\n",
    "    ##########################################################\n",
    "    # READ IMAGES as FLOAT\n",
    "    \n",
    "    if cir:\n",
    "        cir_file_list = os.listdir(root_folder + 'CIR/')\n",
    "        cir_list = []\n",
    "        \n",
    "        for file in cir_file_list:\n",
    "            cir_patch = imageio.imread(root_folder + 'CIR/' + file).astype(np.float32)\n",
    "    \n",
    "            cir_list.append(cir_patch[:,:,:].transpose([2,0,1]))\n",
    "            del cir_patch\n",
    "\n",
    "        cir_features = np.stack(cir_list, axis=0)\n",
    "    \n",
    "    \n",
    "    if rgb:\n",
    "        rgb_file_list = os.listdir(root_folder + 'RGB/')\n",
    "        rgb_list = []\n",
    "        \n",
    "        for file in rgb_file_list:\n",
    "            rgb_patch = imageio.imread(root_folder + 'RGB/' + file).astype(np.float32)\n",
    "    \n",
    "            rgb_list.append(rgb_patch[:,:,:].transpose([2,0,1]))\n",
    "            del rgb_patch\n",
    "        \n",
    "        rgb_features = np.stack(rgb_list, axis=0)\n",
    "\n",
    "\n",
    "    gt_file_list = os.listdir(root_folder + 'GT/')\n",
    "    gt_list = []\n",
    "\n",
    "    for file in gt_file_list:\n",
    "        gt_patch = imageio.imread(root_folder + 'GT/' + file).astype(np.int64)\n",
    " \n",
    "        gt_list.append(gt_patch[:,:])\n",
    "        del gt_patch\n",
    "\n",
    "    \n",
    "    if cir and rgb:\n",
    "        features = np.concatenate([cir_features, rgb_features], axis=1)\n",
    "    elif cir:\n",
    "        features = cir_features\n",
    "    elif rgb:\n",
    "        features = rgb_features\n",
    "    else:\n",
    "        print('No valid data input.')\n",
    "    \n",
    "    ground_truth = np.stack(gt_list, axis=0)\n",
    "\n",
    "    features = torch.from_numpy(features)\n",
    "    features = features.cuda()\n",
    "    ground_truth = torch.from_numpy(ground_truth)\n",
    "    \n",
    "    ########################################################## \n",
    "    return features, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#putting the dataset into the TensorDataset wrapper\n",
    "X, y = read_patch(training_set_path, use_cir, use_rgb)\n",
    "X_t, y_t = read_patch(test_set_path, use_cir, use_rgb)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_t.shape)\n",
    "\n",
    "train_set = tnt.dataset.TensorDataset(list([X, y]))\n",
    "test_set  = tnt.dataset.TensorDataset(list([X_t, y_t]))\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self, n_class, class_names):\n",
    "        self.CM = np.zeros((n_class, n_class))\n",
    "        self.n_class = n_class\n",
    "        self.class_names = class_names\n",
    "  \n",
    "    def clear(self):\n",
    "        self.CM = np.zeros((self.n_class, self.n_class))\n",
    "    \n",
    "    def add_batch(self, gt, pred):\n",
    "        self.CM +=  confusion_matrix(gt, pred, labels = list(range(self.n_class)))\n",
    "    \n",
    "    def overall_accuracy(self):#percentage of correct classification\n",
    "        return 100*self.CM.trace() / self.CM.sum()\n",
    "\n",
    "    def class_IoU(self, show = 1):\n",
    "        ious = np.full(self.n_class, 0.)\n",
    "        for i_class in range(self.n_class):\n",
    "            ious[i_class] = self.CM[i_class, i_class] / \\\n",
    "                (-self.CM[i_class, i_class] \\\n",
    "                + self.CM[i_class, :].sum()\n",
    "                + self.CM[:, i_class].sum())\n",
    "        if show:\n",
    "            print('  |  '.join('{} : {:3.2f}%'.format(name, 100*iou) for name, iou in zip(self.class_names,ious)))\n",
    "        #do not count classes that are not present in the dataset in the mean IoU\n",
    "        return 100*np.nansum(ious) / (np.logical_not(np.isnan(ious))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = ConfusionMatrix(3, class_names)\n",
    "m.add_batch(np.array([0,1,1,1,2,0,0,2,0,2,1]), np.array([0,1,0,0,2,0,1,2,0,2,1]))\n",
    "m.add_batch(np.array([0,1,2,1,2,1,0,2,1]), np.array([0,1,1,1,2,1,0,2,0]))\n",
    "print(m.CM)\n",
    "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
    "m.class_IoU()\n",
    "m.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegNet(nn.Module):\n",
    "    \"\"\"\n",
    "    SegNet network for semantic segmentation\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda = 1):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        n_channels, int, number of input channel\n",
    "        encoder_conv_width, int list, size of the feature maps of convs for the encoder\n",
    "        decoder_conv_width, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(SegNet, self).__init__() #necessary for all classes extending the module class\n",
    "    \n",
    "        assert((encoder_conv_width[3] == encoder_conv_width[5]) \\\n",
    "            and (encoder_conv_width[1] == decoder_conv_width[1]))\n",
    "    \n",
    "        self.maxpool=nn.MaxPool2d(2,2,return_indices=True) #maxpooling layer\n",
    "        self.unpool=nn.MaxUnpool2d(2,2) #unpooling layer\n",
    "    \n",
    "        #encoder\n",
    "        self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[0]),nn.ReLU(True))\n",
    "        self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[1]),nn.ReLU(True))\n",
    "        self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[2]),nn.ReLU(True))\n",
    "        self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[3]),nn.ReLU(True))\n",
    "        self.c5 = nn.Sequential(nn.Conv2d(encoder_conv_width[3],encoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[4]),nn.ReLU(True))\n",
    "        self.c6 = nn.Sequential(nn.Conv2d(encoder_conv_width[4],encoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[5]),nn.ReLU(True))\n",
    "        #decoder\n",
    "        self.c7=nn.Sequential(nn.Conv2d(encoder_conv_width[5]+encoder_conv_width[3],decoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[0]),nn.ReLU(True))\n",
    "        self.c8=nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[1]),nn.ReLU(True))       \n",
    "        self.c9=nn.Sequential(nn.Conv2d(encoder_conv_width[1] + decoder_conv_width[1],decoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[2]),nn.ReLU(True))\n",
    "        self.c10=nn.Sequential(nn.Conv2d(decoder_conv_width[2],decoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[3]),nn.ReLU(True))\n",
    "        #final classifying layer\n",
    "        self.classifier=nn.Conv2d(decoder_conv_width[3],n_class,3,padding=1, padding_mode='reflect')\n",
    "\n",
    "        #weight initialization\n",
    "\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "    \n",
    "        if cuda: #put the model on the GPU memory\n",
    "            self.cuda()\n",
    "    \n",
    "    def init_weights(self,layer): #gaussian init for the conv layers\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        the function called to run inference\n",
    "        \"\"\"  \n",
    "        #encoder\n",
    "        #level 1\n",
    "        x1 = self.c2(self.c1(input))\n",
    "        x2, indices_a_b =self.maxpool(x1)\n",
    "        #level 2\n",
    "        x3=self.c4(self.c3(x2))\n",
    "        x4, indices_b_c =self.maxpool(x3)\n",
    "        #level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        #decoder\n",
    "        #level 2       \n",
    "        y4 = self.unpool(x5, indices_b_c)\n",
    "        y3 = self.c8(self.c7(torch.cat((y4,x3),1)))\n",
    "        #level 1       \n",
    "        y2 = self.unpool(y3, indices_a_b)\n",
    "        y1 = self.c10(self.c9(torch.cat((y2,x1),1)))\n",
    "        #output         \n",
    "        out = self.classifier(y1)\n",
    "    \n",
    "        return out\n",
    "    \n",
    "    \"\"\"\n",
    "    #encoder\n",
    "    self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[0]),nn.ReLU(True))\n",
    "    self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[1]),nn.ReLU(True))\n",
    "    self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[2]),nn.ReLU(True))\n",
    "    self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[3]),nn.ReLU(True))\n",
    "    \n",
    "    self.c9=nn.Sequential(nn.Conv2d(encoder_conv_width[1] + encoder_conv_width[3],decoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[0]),nn.ReLU(True))\n",
    "    self.c10=nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[0]),nn.ReLU(True))\n",
    "    #final classifying layer\n",
    "    self.classifier=nn.Conv2d(decoder_conv_width[1],n_class,3,padding=1, padding_mode='reflect')\n",
    "\n",
    "    #weight initialization\n",
    "\n",
    "    self.c1[0].apply(self.init_weights)\n",
    "    self.c2[0].apply(self.init_weights)\n",
    "    self.c3[0].apply(self.init_weights)\n",
    "    self.c4[0].apply(self.init_weights)\n",
    "    self.c9[0].apply(self.init_weights)\n",
    "    self.c10[0].apply(self.init_weights)\n",
    "    self.classifier.apply(self.init_weights)\n",
    "    \n",
    "    if cuda: #put the model on the GPU memory\n",
    "      self.cuda()\n",
    "    \n",
    "  def init_weights(self,layer): #gaussian init for the conv layers\n",
    "    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "  def forward(self,input):\n",
    "\n",
    "    #encoder\n",
    "    #level 1\n",
    "    x1 = self.c2(self.c1(input))\n",
    "    x2, indices_a_b =self.maxpool(x1)\n",
    "    #level 2\n",
    "    x3=self.c4(self.c3(x2))\n",
    "    \n",
    "    #decoder\n",
    "    #level 1       \n",
    "    y2 = self.unpool(x3, indices_a_b)\n",
    "    y1 = self.c10(self.c9(torch.cat((y2,x1),1)))\n",
    "    #output         \n",
    "    out = self.classifier(y1)\n",
    "    return out\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we consider the first point cloud from the training set\n",
    "tile, gt = read_patch(training_set_path)\n",
    "print(tile.shape)\n",
    "print(gt.shape)\n",
    "print(tile[0:2,:,:,:].shape)\n",
    "segnet = SegNet(3,[4,4,4,4], [4,4],3)\n",
    "print(segnet)\n",
    "print('Total number of parameters: {}'.format(sum([p.numel() for p in segnet.parameters()])))\n",
    "pred = segnet(tile[0:2,:,:,:]) #the None indicate a batch size of 1\n",
    "assert(pred.shape == torch.Size([2,3,500,500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "for index, (tiles, gt) in enumerate(loader):\n",
    "    print(index)\n",
    "    print(tiles.shape)\n",
    "    print(gt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "  \"\"\"train for one epoch\"\"\"\n",
    "  model.train() #switch the model in training mode\n",
    "  \n",
    "  #the loader function will take care of the batching\n",
    "  loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "  #tqdm will provide some nice progress bars\n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  #will keep track of the loss\n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "    optimizer.zero_grad() #put gradient to zero\n",
    "    \n",
    "    pred = model(tiles) #compute the prediction\n",
    "\n",
    "    loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=torch.tensor([0.1,0.3,0.6]))\n",
    "\n",
    "    loss.backward() #compute gradients\n",
    "\n",
    "    for p in model.parameters(): #we clip the gradient at norm 1\n",
    "      p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "    \n",
    "    optimizer.step() #one SGD step\n",
    "    \n",
    "    loss_meter.add(loss.item())\n",
    "    labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "    #need to put the prediction back on the cpu and convert to numpy\n",
    "    cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "    \n",
    "  return cm, loss_meter.value()[0]\n",
    "\n",
    "def eval(model, args):\n",
    "  \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "  model.eval() #switch in eval mode\n",
    "  \n",
    "  loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "  \n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "      \n",
    "      #TODO\n",
    "      pred = model(tiles)\n",
    "      loss = nn.functional.cross_entropy(pred.cpu(),gt)\n",
    "      loss_meter.add(loss.item())\n",
    "      labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "      cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "\n",
    "  return cm, loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "  \"\"\"The full training loop\"\"\"\n",
    "  #initialize the model\n",
    "  \n",
    "  model = SegNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class)\n",
    "\n",
    "  print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "  #define the optimizer\n",
    "  #adam optimizer is always a good guess for classification\n",
    "  optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "  scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[70,90], gamma=0.7)\n",
    "  \n",
    "  TESTCOLOR = '\\033[104m'\n",
    "  TRAINCOLOR = '\\033[100m'\n",
    "  NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "  training_accu=[]\n",
    "  training_miou=[]\n",
    "  training_loss=[]\n",
    "  training_epoch=[]\n",
    "  test_accu=[]\n",
    "  test_miou=[]\n",
    "  test_loss=[]\n",
    "  test_epoch=[]\n",
    "\n",
    "  for i_epoch in range(args.n_epoch):\n",
    "    #train one epoch\n",
    "    cm_train, loss_train = train(model, optimizer, args)\n",
    "    scheduler.step()\n",
    "    print(TRAINCOLOR)\n",
    "    print('Epoch %3d -> Train Overall Accuracy: %3.2f%% Train mIoU : %3.2f%% Train Loss: %1.4f' % (i_epoch, cm_train.overall_accuracy(), cm_train.class_IoU(), loss_train) + NORMALCOLOR)\n",
    "    training_epoch.append(i_epoch)\n",
    "    training_accu.append(cm_train.overall_accuracy())\n",
    "    training_miou.append(cm_train.class_IoU())\n",
    "    training_loss.append(loss_train)\n",
    "\n",
    "    if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "      #periodic testing\n",
    "      cm_test, loss_test = eval(model, args)\n",
    "      print(TESTCOLOR)\n",
    "      print('Test Overall Accuracy: %3.2f%% Test mIoU : %3.2f%%  Test Loss: %1.4f' % (cm_test.overall_accuracy(), cm_test.class_IoU(), loss_test) + NORMALCOLOR)\n",
    "      test_epoch.append(i_epoch)\n",
    "      test_accu.append(cm_test.overall_accuracy())\n",
    "      test_miou.append(cm_test.class_IoU())\n",
    "      test_loss.append(loss_test)\n",
    "      #viewer(n_shown = 1, train = False, model = model, category = 'cigpe', use_mask = False)\n",
    "\n",
    "  plt.figure(figsize=(30, 10))\n",
    "  plt.subplot(1,3,1, ylim=(0,100), xlabel=\"# of epochs\", ylabel=\"Overall Accuracy\")\n",
    "  plt.plot(training_epoch, training_accu)\n",
    "  plt.plot(test_epoch, test_accu)\n",
    "  plt.subplot(1,3,2, ylim=(0,100), xlabel=\"# of epochs\", ylabel=\"mean Class IoU\")\n",
    "  plt.plot(training_epoch, training_miou)\n",
    "  plt.plot(test_epoch, test_miou)\n",
    "  plt.subplot(1,3,3, ylim=(0,1), xlabel=\"# of epochs\", ylabel=\"Loss function\")\n",
    "  plt.plot(training_epoch, training_loss)\n",
    "  plt.plot(test_epoch, test_loss)\n",
    "  plt.show()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "    \n",
    "        pred = model(tiles) #compute the prediction\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt)\n",
    "\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "    \n",
    "        optimizer.step() #one SGD step\n",
    "    \n",
    "        loss_meter.add(loss.item())\n",
    "    \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, args):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "  \n",
    "    loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "    cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "      \n",
    "        #TODO\n",
    "        pred = model(tiles)\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt)\n",
    "        loss_meter.add(loss.item())\n",
    "        labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "        cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "\n",
    "    return cm, loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = SegNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "    #define the optimizer\n",
    "    #adam optimizer is always a good guess for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[70,90], gamma=0.7)\n",
    "  \n",
    "    TESTCOLOR = '\\033[104m'\n",
    "    NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "    test_accu=[]\n",
    "    test_miou=[]\n",
    "    test_loss=[]\n",
    "    test_epoch=[]\n",
    "\n",
    "    for i_epoch in range(args.n_epoch):\n",
    "        #train one epoch\n",
    "        loss_train = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "\n",
    "        if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "            #periodic testing\n",
    "            cm_test, loss_test = eval(model, args)\n",
    "            print(TESTCOLOR)\n",
    "            print('Test Overall Accuracy: %3.2f%% Test mIoU : %3.2f%%  Test Loss: %1.4f' % (cm_test.overall_accuracy(), cm_test.class_IoU(), loss_test) + NORMALCOLOR)\n",
    "            test_epoch.append(i_epoch)\n",
    "            test_accu.append(cm_test.overall_accuracy())\n",
    "            test_miou.append(cm_test.class_IoU())\n",
    "            test_loss.append(loss_test)\n",
    "\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.subplot(1,3,1, ylim=(0,100), xlabel=\"# of epochs\", ylabel=\"Overall Accuracy\")\n",
    "    plt.plot(test_epoch, test_accu)\n",
    "    plt.subplot(1,3,2, ylim=(0,100), xlabel=\"# of epochs\", ylabel=\"mean Class IoU\")\n",
    "    plt.plot(test_epoch, test_miou)\n",
    "    plt.subplot(1,3,3, ylim=(0,1), xlabel=\"# of epochs\", ylabel=\"Loss function\")\n",
    "    plt.plot(test_epoch, test_loss)\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = mock.Mock() #stores the parameters\n",
    "args.n_epoch = 50\n",
    "args.n_epoch_test = int(5) #periodicity of evaluation on test set\n",
    "args.batch_size = 4\n",
    "args.n_class = len(class_names)\n",
    "args.n_channel = 6 if use_cir and use_rgb else 3\n",
    "args.conv_width = [8,16,8,8,8,8]\n",
    "args.dconv_width = [8,16,8,8]\n",
    "args.cuda = 1\n",
    "args.lr = 1e-3\n",
    "\n",
    "a = time()\n",
    "trained_model = train_full(args)\n",
    "b = time()\n",
    "\n",
    "print('Training finished in ' + str(b-a) + 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_t.shape)\n",
    "print(y_t.shape)\n",
    "\n",
    "print(X_t.max())\n",
    "print(X_t.min())\n",
    "print(y_t.max())\n",
    "print(y_t.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb_cir_gt_pred(tile_index, data, gt, model, cir, rgb):\n",
    "    # Function to plot prediction vs ground truth\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(facecolor='white')\n",
    "\n",
    "    data = data[tile_index,:,:,:]\n",
    "    pred = model(data[None,:,:,:]).cpu().detach().numpy()\n",
    "    pred = pred[0,:,:,:].argmax(0).squeeze()\n",
    "    \n",
    "    unique, counts = np.unique(pred, return_counts=True)\n",
    "    print(dict(zip(unique, counts)))\n",
    "    \n",
    "    data = data.cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    if cir and rgb:\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(data[:3].transpose([1,2,0]))\n",
    "        plt.title('NIR Red Green composite')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 2)\n",
    "        plt.imshow(data[-3:].transpose([1,2,0]))\n",
    "        plt.title('Red Green Blue composite')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 4, 3)\n",
    "        plt.imshow(gt[tile_index,:,:], CMAP)\n",
    "        plt.title('GT Labels')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 4, 4)\n",
    "        plt.imshow(pred, CMAP)\n",
    "        plt.title('Predicted Labels')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    elif cir or rgb:\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(data.transpose([1,2,0]))\n",
    "        if cir:\n",
    "            plt.title('NIR Red Green composite')\n",
    "        else:\n",
    "            plt.title('Red Green Blue composite')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(gt[tile_index,:,:], CMAP)\n",
    "        plt.title('GT Labels')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred, CMAP)\n",
    "        plt.title('Predicted Labels')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_rgb_cir_gt_pred(3, X_t, y_t, trained_model, use_cir, use_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
