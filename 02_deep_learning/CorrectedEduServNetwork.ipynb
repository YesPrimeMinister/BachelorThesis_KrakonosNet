{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "23eyw-b_-zGO"
   },
   "source": [
    "# Correction of the convolutional encoder-decoder practical exercise\n",
    "\n",
    "by Loic Landrieu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzSRII3t-0Du"
   },
   "outputs": [],
   "source": [
    "#[1] import and installations\n",
    "import torch\n",
    "import numpy as np\n",
    "import torchnet as tnt\n",
    "import functools\n",
    "import mock\n",
    "import math\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch.nn.functional as nnf\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_txgFGmR5p7"
   },
   "outputs": [],
   "source": [
    "#[2] Authentification\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "downloaded = drive.CreateFile({'id':'1qB9Gt9UCTokkSM4Fvy-srQUU9ir0Bke7'})\n",
    "downloaded.GetContentFile('landCover.hdf5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzPkTlHJR54M"
   },
   "outputs": [],
   "source": [
    "#[4] building the train and test sets\n",
    "data_file = h5py.File(\"landCover.hdf5\",'r')\n",
    "train_obs = data_file['train_observation'][:]\n",
    "train_gt = data_file['train_gt'][:]\n",
    "test_obs = data_file['test_observation'][:]\n",
    "test_gt = data_file['test_gt'][:]\n",
    "n_train = train_obs.shape[0]\n",
    "n_test = test_obs.shape[0]\n",
    "class_names = [\"Urban\", \"Water\", \"Fields\", \"Road\", \"Vegetation\", \"Buildings\"]\n",
    "\n",
    "\n",
    "print(\"%d tiles for training, %d tiles for testing\" % (n_train, n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qVEVgJ_uR56z"
   },
   "outputs": [],
   "source": [
    "#[4] data loader\n",
    "\n",
    "def augment(obs, gt):\n",
    "  \"\"\"augmentation function\n",
    "  Leave untouched for now until question XX\n",
    "  \"\"\"\n",
    "  return obs, gt#does nothing, leaves like that for now\n",
    "  #random gaussian noise\n",
    "  sigma, clip= 0.01, 0.02 # https://github.com/charlesq34/pointnet/blob/master/provider.py#L74\n",
    "  obs = obs + np.clip(sigma * np.random.randn(*obs.shape), -1*clip, clip).astype(np.float32)\n",
    "  #random rotation 0 90 180 270 degree\n",
    "  #n_turn =  np.random.randint(4)   #removed because detrmental, see Q14\n",
    "  #obs = np.rot90(obs, k=n_turn, axes=(1,2)).copy()\n",
    "  #gt = np.rot90(gt, k=n_turn, axes=(0,1)).copy()\n",
    "  return obs, gt\n",
    "\n",
    "\n",
    "def tile_loader(tile_index, train = True, cuda = 1):\n",
    "  \"\"\"\n",
    "  load a tile and returns the obseravtion and associated ground truth\n",
    "  INPUT:\n",
    "  tile_index = int, index of the tile\n",
    "  train = int, train = 1 iff in the train set\n",
    "  cuda = int, cuda = 1 if using GPUs\n",
    "  OUTPUT\n",
    "  obs, [256 x 256 x 4] float array containing the observation\n",
    "  gt, [256 x 256] uint8 array, containing the pixels semantic labels  \n",
    "  \"\"\"\n",
    "  if train:\n",
    "    obs = train_obs[tile_index,:,:,:].transpose(2,0,1) #put channels first\n",
    "    gt = train_gt[tile_index,:,:]\n",
    "  else:\n",
    "    obs = test_obs[tile_index,:,:,:].transpose(2,0,1)\n",
    "    gt = test_gt[tile_index,:,:]\n",
    "    \n",
    "  if train: #augmentation - for training only\n",
    "    obs, gt = augment(obs, gt)   \n",
    "  \n",
    "  obs = torch.from_numpy(obs)\n",
    "  gt = torch.from_numpy(gt)\n",
    "  \n",
    "  if cuda:\n",
    "    obs = obs.cuda()\n",
    "   \n",
    "  return obs, gt.long()\n",
    "\n",
    "#putting the dataset into the ListDataset wrapper\n",
    "test_set  = tnt.dataset.ListDataset(list(range(n_test)),functools.partial(tile_loader, train=False))\n",
    "train_set = tnt.dataset.ListDataset(list(range(n_train)),functools.partial(tile_loader, train=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0D7srfjR59f"
   },
   "outputs": [],
   "source": [
    "#[5] functions used for visualization\n",
    "def view_rgb(tile, ax = None):\n",
    "  \"\"\" show the rgb values of the tile in figure ax\"\"\"\n",
    "  if ax==None:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "  tile_corrected = np.minimum(np.maximum(tile[:3,:,], 0), 1) #normalization\n",
    "  ax.imshow(tile_corrected.transpose(0,2).transpose(0,1)) #put channels back as dim 3\n",
    "  plt.axis('off')\n",
    "  \n",
    "def view_infrared(tile, ax = None):\n",
    "  \"\"\" show the infrared tile in figure ax\"\"\"\n",
    "  if ax==None:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "  tile_corrected = np.minimum(np.maximum(tile[3,:,:], 0), 1) #normalization\n",
    "  ax.imshow(tile_corrected,cmap='hot')\n",
    "  plt.axis('off')\n",
    "\n",
    "def view_labels(label, ax = None, mask = None):\n",
    "  \"\"\" show the ground truth with a colorcode corresponding to labels\"\"\"\n",
    "  if ax==None:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "  if mask is not None:\n",
    "    label[mask] = 99\n",
    "  n_pixel = label.shape[1]\n",
    "  colors = np.zeros((n_pixel,n_pixel,3))\n",
    "  colors[np.where(label==99)] = [1  ,1  ,1  ] #not labelled\n",
    "  colors[np.where(label==0)]  = [1  ,0.8,0.8] #building limit\n",
    "  colors[np.where(label==1)]  = [0  ,0  ,1  ] #water\n",
    "  colors[np.where(label==2)]  = [0.9,0.9,0  ] #fields\n",
    "  colors[np.where(label==3)]  = [0.5,0.5,0.5] #road\n",
    "  colors[np.where(label==4)]  = [0  ,.8  ,0  ] #vegetation\n",
    "  colors[np.where(label==5)]  = [1,  0  ,0  ] #building\n",
    "  ax.imshow(colors)\n",
    "  plt.axis('off')\n",
    "  \n",
    "def view_error(pred, gt, ax = None):\n",
    "  \"\"\" show the error between pred and gt with colorcode:\n",
    " green when 'gt'='pred', red when 'gt'!='pred' and black\n",
    " when unnannotated (gt = 0)\"\"\"\n",
    "  if ax==None:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "  n_pixel = gt.shape[1]\n",
    "  colors = np.zeros((n_pixel,n_pixel,3))\n",
    "  colors[np.where(pred==gt.squeeze())] = [0, 1, 0] #correct prediction\n",
    "  colors[np.where(pred!=gt.squeeze())] = [1, 0, 0] #error\n",
    "  colors[np.where(gt.squeeze()==99)] = [0, 0, 0]   #unannotated \n",
    "  ax.imshow(colors)\n",
    "  plt.axis('off')\n",
    "  \n",
    "def viewer(n_shown = 3, category = 'cig', train = True, model = None, use_mask = False):\n",
    "  \"\"\" plot 'n_shown' random tiles train/test set with the following visuals:\n",
    "  if 'c' in category : rgb color\n",
    "  if 'i' in category : infrared\n",
    "  if 'g' in category : ground truth\n",
    "  if 'p' in category : prediction\n",
    "  if 'e' in category : error\n",
    "  Note that for 'p' or 'e' ou need to add a trained model as input\n",
    "  \n",
    "  \"\"\"\n",
    "  n_category = len(category) #number of types of image to show\n",
    "  fig = plt.figure(figsize=(n_category * 5, n_shown * 5)) #adapted dimension\n",
    "  \n",
    "  subplot_index = 1 #keep track of current subplot\n",
    "  \n",
    "  #chose random tiles\n",
    "  tile_indices = np.random.choice(n_train, n_shown) if train \\\n",
    "  else np.random.choice(n_test, n_shown)\n",
    "  \n",
    "  for tile_index in tile_indices:\n",
    "    \n",
    "    tile, gt = tile_loader(tile_index, train = train, cuda=0)\n",
    "    \n",
    "    if 'c' in category:\n",
    "      ax = fig.add_subplot(n_shown, n_category, subplot_index, aspect='equal')\n",
    "      if subplot_index <= n_category : \n",
    "        ax.set(title='RGB')\n",
    "      view_rgb(tile, ax = ax)\n",
    "      subplot_index += 1\n",
    "    if 'i' in category:  \n",
    "      ax = fig.add_subplot(n_shown, n_category, subplot_index, aspect='equal')\n",
    "      if subplot_index <= n_category : \n",
    "        ax.set(title='Infrared')\n",
    "      view_infrared(tile, ax = ax)\n",
    "      subplot_index += 1\n",
    "    if 'g' in category:  \n",
    "      ax = fig.add_subplot(n_shown, n_category, subplot_index, aspect='equal')\n",
    "      if subplot_index <= n_category : \n",
    "        ax.set(title='Ground Truth')\n",
    "      view_labels(gt, ax = ax)\n",
    "      subplot_index += 1\n",
    "    if 'p' in category:  \n",
    "      ax = fig.add_subplot(n_shown, n_category, subplot_index, aspect='equal')\n",
    "      if subplot_index <= n_category : \n",
    "        ax.set(title='Prediction')\n",
    "      pred = model(tile[None,:,:,:].cuda()).cpu().argmax(1).squeeze()\n",
    "      if use_mask:\n",
    "        pred[gt==99] = 99\n",
    "      view_labels(pred, ax = ax)\n",
    "      subplot_index += 1  \n",
    "    if 'e' in category: \n",
    "      ax = fig.add_subplot(n_shown, n_category, subplot_index, aspect='equal')\n",
    "      if subplot_index <= n_category : \n",
    "        ax.set(title='Error')\n",
    "      pred = model(tile[None,:,:,:].cuda()).cpu().argmax(1).squeeze()\n",
    "      view_error(pred, gt, ax = ax)\n",
    "      subplot_index += 1\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7iyfh-NR5_u"
   },
   "outputs": [],
   "source": [
    "#[6]\n",
    "viewer(n_shown = 3, category = 'cig', train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17Ohx9RVh6Ze"
   },
   "outputs": [],
   "source": [
    "#[7]\n",
    "class ConfusionMatrix:\n",
    "  def __init__(self, n_class, class_names):\n",
    "    self.CM = np.zeros((n_class, n_class))\n",
    "    self.n_class = n_class\n",
    "    self.class_names = class_names\n",
    "  \n",
    "  def clear(self):\n",
    "    self.CM = np.zeros((self.n_class, self.n_class))\n",
    "    \n",
    "  def add_batch(self, gt, pred):\n",
    "    self.CM +=  confusion_matrix(gt, pred, labels = list(range(self.n_class)))\n",
    "    \n",
    "  def overall_accuracy(self):#percentage of correct classification\n",
    "    return 100*self.CM.trace() / self.CM.sum()\n",
    "\n",
    "  def class_IoU(self, show = 1):\n",
    "    ious = np.full(self.n_class, 0.)\n",
    "    for i_class in range(self.n_class):\n",
    "      ious[i_class] = self.CM[i_class, i_class] / \\\n",
    "        (-self.CM[i_class, i_class] \\\n",
    "        + self.CM[i_class, :].sum()\n",
    "        + self.CM[:, i_class].sum())\n",
    "    if show:\n",
    "      print('  |  '.join('{} : {:3.2f}%'.format(name, 100*iou) for name, iou in zip(self.class_names,ious)))\n",
    "    #do not count classes that are not present in the dataset in the mean IoU\n",
    "    return 100*np.nansum(ious) / (np.logical_not(np.isnan(ious))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJPZG2yyh7VG"
   },
   "outputs": [],
   "source": [
    "#[8]\n",
    "m = ConfusionMatrix(6, class_names)\n",
    "m.add_batch(np.array([0,1,1,5,2,0,0,4,0,5,3]), np.array([0,1,0,5,2,0,1,4,0,5,3]))\n",
    "m.add_batch(np.array([0,1,5,1,2,1,0,2,3]), np.array([0,1,1,1,2,1,0,2,3]))\n",
    "print(m.CM)\n",
    "print(\"OA = %3.2f%%\" % (m.overall_accuracy()))\n",
    "m.class_IoU()\n",
    "m.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jGJ7_1In_H7Q"
   },
   "source": [
    "$\\textbf{Why is it necessary that d4 = d6 and d2 = d8?}$\n",
    "\n",
    "The maxpool indices used to go from $x_1$ to $x_2$ (width $d_2$) must be compatible with the unpool layer used to go from $y_3$ to $y_2$ (width $d_8$), and same for the second maxpool/unpool pair.\n",
    "\n",
    "Many people mention concatenation, but there is no constraint on width when concatenating tensors: you can concatenate two tensors of size $H_1\\times W_1 \\times D_1$ and $H_2\\times W_2 \\times D_2$ for arbitrary width $D_1$ and $D_2$, as long as $H_1=H_2$ and $W_1=W_2$. The feature map size is not controlled by the convolution width, but by the organisation of maxpools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDH7IhVwz3s8"
   },
   "outputs": [],
   "source": [
    "#[9]\n",
    "class SegNet(nn.Module):\n",
    "  \"\"\"\n",
    "  SegNet network for semantic segmentation\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda = 1):\n",
    "    \"\"\"\n",
    "    initialization function\n",
    "    n_channels, int, number of input channel\n",
    "    encoder_conv_width, int list, size of the feature maps of convs for the encoder\n",
    "    decoder_conv_width, int list, size of the feature maps of convs for the decoder\n",
    "    n_class = int,  the number of classes\n",
    "    \"\"\"\n",
    "    super(SegNet, self).__init__() #necessary for all classes extending the module class\n",
    "    \n",
    "    assert((encoder_conv_width[3] == encoder_conv_width[5]) \\\n",
    "     and (encoder_conv_width[1] == decoder_conv_width[1]))\n",
    "    \n",
    "    self.maxpool=nn.MaxPool2d(2,2,return_indices=True) #maxpooling layer\n",
    "    self.unpool=nn.MaxUnpool2d(2,2) #unpooling layer\n",
    "    #encoder\n",
    "    self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[0]),nn.ReLU(True))\n",
    "    self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[1]),nn.ReLU(True))\n",
    "    self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[2]),nn.ReLU(True))\n",
    "    self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[3]),nn.ReLU(True))\n",
    "    self.c5 = nn.Sequential(nn.Conv2d(encoder_conv_width[3],encoder_conv_width[4],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[4]),nn.ReLU(True))\n",
    "    self.c6 = nn.Sequential(nn.Conv2d(encoder_conv_width[4],encoder_conv_width[5],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(encoder_conv_width[5]),nn.ReLU(True))\n",
    "    #decoder\n",
    "    self.c7=nn.Sequential(nn.Conv2d(encoder_conv_width[5]+encoder_conv_width[3],decoder_conv_width[0],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(decoder_conv_width[0]),nn.ReLU(True))\n",
    "    self.c8=nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(decoder_conv_width[1]),nn.ReLU(True))       \n",
    "    self.c9=nn.Sequential(nn.Conv2d(encoder_conv_width[1] + decoder_conv_width[1],decoder_conv_width[2],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(decoder_conv_width[2]),nn.ReLU(True))\n",
    "    self.c10=nn.Sequential(nn.Conv2d(decoder_conv_width[2],decoder_conv_width[3],3,padding=1, padding_mode='reflection'),nn.BatchNorm2d(decoder_conv_width[3]),nn.ReLU(True))\n",
    "    #final classifying layer\n",
    "    self.classifier=nn.Conv2d(decoder_conv_width[3],n_class,3,padding=1, padding_mode='reflection')\n",
    "\n",
    "    #weight initialization\n",
    "\n",
    "    self.c1[0].apply(self.init_weights)\n",
    "    self.c2[0].apply(self.init_weights)\n",
    "    self.c3[0].apply(self.init_weights)\n",
    "    self.c4[0].apply(self.init_weights)\n",
    "    self.c5[0].apply(self.init_weights)\n",
    "    self.c6[0].apply(self.init_weights)\n",
    "    self.c7[0].apply(self.init_weights)\n",
    "    self.c8[0].apply(self.init_weights)\n",
    "    self.c9[0].apply(self.init_weights)\n",
    "    self.c10[0].apply(self.init_weights)\n",
    "    self.classifier.apply(self.init_weights)\n",
    "    \n",
    "    if cuda: #put the model on the GPU memory\n",
    "      self.cuda()\n",
    "    \n",
    "  def init_weights(self,layer): #gaussian init for the conv layers\n",
    "    nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "  def forward(self,input):\n",
    "    \"\"\"\n",
    "    the function called to run inference\n",
    "    \"\"\"  \n",
    "    #encoder\n",
    "    #level 1\n",
    "    x1 = self.c2(self.c1(input))\n",
    "    x2, indices_a_b =self.maxpool(x1)\n",
    "    #level 2\n",
    "    x3=self.c4(self.c3(x2))\n",
    "    x4, indices_b_c =self.maxpool(x3)\n",
    "    #level 3\n",
    "    x5 = self.c6(self.c5(x4))\n",
    "    #decoder\n",
    "    #level 2       \n",
    "    y4 = self.unpool(x5, indices_b_c)\n",
    "    y3 = self.c8(self.c7(torch.cat((y4,x3),1)))\n",
    "    #level 1       \n",
    "    y2 = self.unpool(y3, indices_a_b)\n",
    "    y1 = self.c10(self.c9(torch.cat((y2,x1),1)))\n",
    "    #output         \n",
    "    out = self.classifier(y1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHQ9eipjDxs4"
   },
   "outputs": [],
   "source": [
    "#[10]\n",
    "#==================TEST===============================\n",
    "#we consider the first point cloud from the training set\n",
    "tile, gt = tile_loader(0)\n",
    "segnet = SegNet(4,[16,16,32,32,64,32], [32,16,32,16],6)\n",
    "print(segnet)\n",
    "print('Total number of parameters: {}'.format(sum([p.numel() for p in segnet.parameters()])))\n",
    "pred = segnet(tile[None,:,:,:]) #the None indicate a batch size of 1\n",
    "assert(pred.shape == torch.Size([1,6,256,256]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KIOWMc9PR6BT"
   },
   "outputs": [],
   "source": [
    "#[11]\n",
    "def train(model, optimizer, args):\n",
    "  \"\"\"train for one epoch\"\"\"\n",
    "  model.train() #switch the model in training mode\n",
    "  \n",
    "  #the loader function will take care of the batching\n",
    "  loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "  #tqdm will provide some nice progress bars\n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  #will keep track of the loss\n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "    optimizer.zero_grad() #put gradient to zero\n",
    "    \n",
    "    pred = model(tiles) #compute the prediction\n",
    "\n",
    "    loss = nn.functional.cross_entropy(pred.cpu(),gt, ignore_index=99)\n",
    "\n",
    "    loss.backward() #compute gradients\n",
    "\n",
    "    for p in model.parameters(): #we clip the gradient at norm 1\n",
    "      p.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step() #one SGD step\n",
    "    \n",
    "    loss_meter.add(loss.item())\n",
    "    labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "    #need to put the prediction back on the cpu and convert to numpy\n",
    "    cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "    \n",
    "  return cm, loss_meter.value()[0]\n",
    "\n",
    "def eval(model, args):\n",
    "  \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "  model.eval() #switch in eval mode\n",
    "  \n",
    "  loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "  \n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "      \n",
    "      pred = model(tiles) #compute the prediction\n",
    "\n",
    "      loss = nn.functional.cross_entropy(pred.cpu(),gt, ignore_index=99)\n",
    "      \n",
    "      loss_meter.add(loss.item())\n",
    "      labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "      #need to put the prediction back on the cpu and convert to numpy\n",
    "      cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "  return cm, loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "  \"\"\"The full training loop\"\"\"\n",
    "  #initialize the model\n",
    "  \n",
    "  model = SegNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class)\n",
    "\n",
    "  print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "  #define the optimizer\n",
    "  #adam optimizer is always a good guess for classification\n",
    "  optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "  \n",
    "  TESTCOLOR = '\\033[104m'\n",
    "  TRAINCOLOR = '\\033[100m'\n",
    "  NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "  for i_epoch in range(args.n_epoch):\n",
    "    #train one epoch\n",
    "    cm_train, loss_train = train(model, optimizer, args)\n",
    "    print(TRAINCOLOR)\n",
    "    print('Epoch %3d -> Train Overall Accuracy: %3.2f%% Train mIoU : %3.2f%% Train Loss: %1.4f' % (i_epoch, cm_train.overall_accuracy(), cm_train.class_IoU(), loss_train) + NORMALCOLOR)\n",
    "\n",
    "    if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "      #periodic testing\n",
    "      cm_test, loss_test = eval(model, args)\n",
    "      print(TESTCOLOR)\n",
    "      print('Test Overall Accuracy: %3.2f%% Test mIoU : %3.2f%%  Test Loss: %1.4f' % (cm_test.overall_accuracy(), cm_test.class_IoU(), loss_test) + NORMALCOLOR)\n",
    "      viewer(n_shown = 1, train = False, model = model, category = 'cigpe', use_mask = False)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i5D1_H5pR6DL"
   },
   "outputs": [],
   "source": [
    "#[12]\n",
    "args = mock.Mock() #stores the parameters\n",
    "args.n_epoch = 50\n",
    "args.n_epoch_test = int(1) #periodicity of evaluation on test set\n",
    "args.batch_size = 16\n",
    "args.n_class = 6\n",
    "args.n_channel = 4\n",
    "args.conv_width = [16,16,32,32,64,32]\n",
    "args.dconv_width = [32,16,32,16]\n",
    "args.cuda = 1\n",
    "args.lr = 5e-3\n",
    "\n",
    "trained_model = train_full(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ycFT8frLBBsr"
   },
   "source": [
    "$\\textbf{Comment on the evolution of the performance on the training set and test set.}$\n",
    "\n",
    "Train accuracy is higher than test accuracy, test accuracy fluctuates a lot.\n",
    "\n",
    "$\\textbf{Propose an explanation as to why the pink class surrounding the buildings was added.}$\n",
    "\n",
    "The pink class helps the network to retrieve believabe shape for buildings. If one were to remove this class (set it to 99, or 5, an easy change in the loader function), they would observe that the buildings in the same city blocks would be merged into one big blob. This is because in the ground truth, the area in between buildings is unannotated, meaning that the network is not penalized no matter what it predicts there.\n",
    "\n",
    "By adding a thin outline around buildings, the networks produces delimitated builings, and fill in between wih the pink class 'urban'. Note that removing the urban class should not impact the actual prediction score because it affects unannotated pixels which are not counted in the scores, but impact the visual quality of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWiLoVYR06t0"
   },
   "outputs": [],
   "source": [
    "#[13]\n",
    "viewer(n_shown=5, category = 'cigpe', model = trained_model, train = False, use_mask = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2UqLdFMbR6FD"
   },
   "outputs": [],
   "source": [
    "#[14]\n",
    "def view_embeddings(fmap, ax = None):\n",
    "  if ax==None:\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(1, 1, 1, aspect='equal')\n",
    "  fmap_dim = fmap.shape[1]\n",
    "  n_pix = fmap.shape[2]\n",
    "  #we use a pca to project the emebddings to a RGB space\n",
    "  pca = PCA(n_components=3)\n",
    "  pca.fit(np.eye(fmap_dim))\n",
    "  #we need to adapt dimension and memory allocation to CPU\n",
    "  fmap_ = fmap.cpu().detach().numpy().squeeze().reshape((fmap_dim, n_pix * n_pix)).transpose(1,0)\n",
    "  color = pca.transform(fmap_)\n",
    "  #we normalize for visibility\n",
    "  color = np.maximum(np.minimum(((color - color.mean(1, keepdims = True) +0.5) / (2 * color.std(1, keepdims = True))), 1), 0)\n",
    "  color = color.reshape((n_pix, n_pix,3), order= 'C')\n",
    "  ax.imshow(color)\n",
    "  plt.axis('off')\n",
    "\n",
    "def view_U(model, tile_index = None, train = False):\n",
    "  if tile_index is None:\n",
    "    tile_index = np.random.randint(n_train) if train \\\n",
    "    else np.random.randint(n_test)\n",
    "  tile, gt = tile_loader(tile_index, train = train, cuda=1)\n",
    "    \n",
    "  input = tile[None,:,:,:]\n",
    "  x1 = model.c2(model.c1(input))\n",
    "  x2, indices_1_2 = model.maxpool(x1)\n",
    "  #level 2\n",
    "  x3 = model.c4(model.c3(x2))\n",
    "  x4, indices_2_3 = model.maxpool(x3)\n",
    "  #level 3\n",
    "  x5 = model.c6(model.c5(x4))\n",
    "  #level 2       \n",
    "  y4 = model.unpool(x5, indices_2_3,x2.size())\n",
    "  y3 = model.c8(model.c7(torch.cat((y4,x3),1)))\n",
    "  #level 1       \n",
    "  y2 = model.unpool(y3, indices_1_2,x1.size())\n",
    "  y1 = model.c10(model.c9(torch.cat((y2,x1),1)))\n",
    "  #output       \n",
    "  out = model.classifier(y1)\n",
    "  pred = out.argmax(1)\n",
    "\n",
    "  fig = plt.figure(figsize=(25, 10)) #adapted dimension\n",
    "  ax = fig.add_subplot(3, 7, 1, aspect=1)\n",
    "  ax.set(title='Input : %d x %d x %d' %(tile.shape))\n",
    "  view_rgb(tile.cpu(), ax)\n",
    "  ax = fig.add_subplot(3, 7, 2, aspect=1)\n",
    "  ax.set(title='x1 : %d x %d x %d' %(x1.shape[1:]))\n",
    "  view_embeddings(x1, ax)\n",
    "  ax = fig.add_subplot(3, 7, 9, aspect=1)\n",
    "  ax.set(title='x2 : %d x %d x %d' %(x2.shape[1:]))\n",
    "  view_embeddings(x2, ax)\n",
    "  ax = fig.add_subplot(3, 7, 10, aspect=1)\n",
    "  ax.set(title='x3 : %d x %d x %d' %(x3.shape[1:]))\n",
    "  view_embeddings(x3, ax)\n",
    "  ax = fig.add_subplot(3, 7, 17, aspect=1)\n",
    "  ax.set(title='x4 : %d x %d x %d' %(x4.shape[1:]))\n",
    "  view_embeddings(x4, ax)\n",
    "  ax = fig.add_subplot(3, 7, 18, aspect=1)\n",
    "  ax.set(title='x5 : %d x %d x %d' %(x5.shape[1:]))\n",
    "  view_embeddings(x5, ax)\n",
    "  ax = fig.add_subplot(3, 7, 11, aspect=1)\n",
    "  ax.set(title='y4 : %d x %d x %d' %(y4.shape[1:]))\n",
    "  view_embeddings(y4, ax)\n",
    "  ax = fig.add_subplot(3, 7, 12, aspect=1)\n",
    "  ax.set(title='y3 : %d x %d x %d' %(y3.shape[1:]))\n",
    "  view_embeddings(y3, ax)\n",
    "  ax = fig.add_subplot(3, 7, 5, aspect=1)\n",
    "  ax.set(title='y2 : %d x %d x %d' %(y2.shape[1:]))\n",
    "  view_embeddings(y2, ax)\n",
    "  ax = fig.add_subplot(3, 7, 6, aspect=1)\n",
    "  ax.set(title='y1 : %d x %d x %d' %(y1.shape[1:]))\n",
    "  view_embeddings(y1, ax)\n",
    "  ax = fig.add_subplot(3, 7, 7, aspect=1)\n",
    "  ax.set(title='Output : 6 x %d x %d' %(tile.shape[1:]))\n",
    "  view_labels(pred.cpu().detach().numpy().squeeze(), ax, mask=gt==99)\n",
    "  ax = fig.add_subplot(3, 7, 14, aspect=1)\n",
    "  ax.set(title='Ground Truth : 6 x %d x %d' %(tile.shape[1:]))\n",
    "  view_labels(gt, ax)\n",
    "  ax = fig.add_subplot(3, 7, 21, aspect=1)\n",
    "  ax.set(title='Error')\n",
    "  view_error(pred.cpu().detach().numpy().squeeze(), gt.numpy(), ax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6zw7UZZBEJF"
   },
   "outputs": [],
   "source": [
    "#[15]]\n",
    "view_U(trained_model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0hGpLDGBBuM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kS6I_T6QCo2q"
   },
   "source": [
    "$\\textbf{Q11}:$ large batches produce more accurate gradients and less iterations per epoch, and should be used with larger learning rate. Vice versa.\n",
    "\n",
    "This exercise works with a large spectrum of batch sizes as long as you adjust the learning rate in consequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZeQ9u7y9Co6M"
   },
   "source": [
    "$\\textbf{Q12}:$ Drop out shouldn't help too much, as the batch norms already robustigy the network. In general, excep if you observe itense overfitting, don't use dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDncTuJPCo_I"
   },
   "source": [
    "$\\textbf{Q13}:$ should produce a small increase in IoU for the cost of a small decrease in OA. It makes sense since the unweighted cross-entropy is a surrogate for the OA, in which all classes are treated equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVOOX_kMCpL7"
   },
   "source": [
    "$\\textbf{Q14}:$ Jittering (Gaussian noise) should help a little bit to robustify the network, but marginally as the data are not so noisy to start with.\n",
    "\n",
    "Random rotations on the other hand decreases performance by a lot, for a nontrivial reason. Since Spot6 is heliosynchronous, shadows of buildings are always on the same side of buildings. This allows the newtwork to get cues on the height of buidlings. By adding a random rotation, it makes this height regression much harder. This is a prime example of (geometric) information leaking accidentaly.\n",
    "\n",
    "So, should we do a random rotation? If we only want to use Spot6 in a small area, then we might as well exploit the capacity of the network to expoit the shadows and remove the random rotation. If we want to apply the samenetwork to different areas / satelite, then we should add the random rotation to prevent the network from learning area-specific informations.\n",
    "\n",
    "I saw many people saying that we should duplicate the data. This is not true and should be avoided at all cost. To add a random rotation (0,90,180 or 270 degrees) you just need to add this operation randomly, on-the-fly in the loader function. This will act exactly the same way that if you duplicated your training set by 4 and added the rotations there. But with the latter you now need 4 times as much disk space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IEcH3I2CpO_"
   },
   "source": [
    "$\\textbf{Q15}:$ networks too large tends to overfit. Training performance should increase monotonically with network size, while the test performance should start to decrease at some point.\n",
    "\n",
    "The default configuration is pretty wel chosen, but some of you had good results with slightly larger networks.\n",
    "\n",
    "Many people commplained about memory crashes and blamed google collab. The GPU on google are TeslaV100 with 16GB of memory, and which cost over 3000$. So this is not the problem.\n",
    "\n",
    "If you try very large networks, you should decrease the batch size so that it fits in memory. There are other tricks like memory mongering which can help. But for such an easy task, you don't need such alrge networks. 200.000 parameters should be the absolute maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "km2xozM7EBQZ"
   },
   "source": [
    "$\\textbf{Q16}:$ Should marginally help. Allows to, use very large initial learning rate (0.05) which are subsequentialy decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGVkyIadEBTe"
   },
   "source": [
    "$\\textbf{Q17}:$ straihgtforward. Should not help too much because more stages mostly increases the receptive field, which is large enough with 2 stages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcDqqcSLEBWr"
   },
   "source": [
    "$\\textbf{Q18}:$ should help a lot to stabilize the test performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_w2BeKOvEBbw"
   },
   "source": [
    "$\\textbf{Q19}:$ straightfoward from Q18. Pseudocode:\n",
    "```\n",
    "input = args # a configuration to evaluate\n",
    "performance = empty #the chosen metric\n",
    "p = random permutation\n",
    "size_fold = size_dataset / nfold\n",
    "for i in range(nfold)\n",
    "  train = full_dataset[p[i*size_fold: (i+1)*size_fold]]\n",
    "  test = the rest\n",
    "  full_train(train, args) #train a configuration config on train\n",
    "  performance += evaluation(test) \n",
    "performance = performance / nfold #average of the performance on the whole dataset with no leakage\n",
    "return performance\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG7uoaLBEBg3"
   },
   "source": [
    "$\\textbf{Q20}:$ could potentially help a little, but mostly for the visual quality of the prediction.\n",
    "\n",
    "Loss annealing should be used because the output of the newtork will be initially very noisy, and a TV prior on random noise will slow down learning (will push towards an uniform prediction). Once the network gets confident enough, the TV prior can intervene and help increase spatial regularity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sl10NVUU-0LU"
   },
   "outputs": [],
   "source": [
    "def compute_adjacency_graph(n = 256, cuda = 1):\n",
    "  \"\"\"\n",
    "  compute the 4-adjacency graph for a square image of size n\n",
    "  \"\"\"\n",
    "  edges = np.zeros((2*n**2,4), dtype='int64')\n",
    "  index_edg = 0\n",
    "  for i in range(n):\n",
    "    for j in range(n):\n",
    "      if i < n-1:\n",
    "        edges[index_edg,:] = [i,j,i+1,j]\n",
    "        index_edg += 1\n",
    "      if j < n-1:\n",
    "        edges[index_edg,:] = [i,j,i,j+1]\n",
    "        index_edg += 1\n",
    "  return edges[:index_edg,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cs2e1dqgR6Ha"
   },
   "outputs": [],
   "source": [
    "def TV(x,edges):\n",
    "  return torch.abs(x[:,:,edges[:,0],edges[:,1]] - x[:,:,edges[:,2],edges[:,3]]).sum() / edges.shape[0] / x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DXu0y0c0R6I-"
   },
   "outputs": [],
   "source": [
    "#[11]\n",
    "def train_TV(model, optimizer, edges, weight_TV, args):\n",
    "  \"\"\"train for one epoch\"\"\"\n",
    "  model.train() #switch the model in training mode\n",
    "  \n",
    "  #the loader function will take care of the batching\n",
    "  loader = torch.utils.data.DataLoader(train_set, \\\n",
    "         batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "  #tqdm will provide some nice progress bars\n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  #will keep track of the loss\n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  loss_acc_meter = tnt.meter.AverageValueMeter()\n",
    "  loss_TV_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "    optimizer.zero_grad() #put gradient to zero\n",
    "    \n",
    "    pred = model(tiles) #compute the prediction\n",
    "\n",
    "    loss_acc = nn.functional.cross_entropy(pred.cpu(),gt, ignore_index=99)\n",
    "    loss_TV = TV(pred.cpu(), edges)\n",
    "\n",
    "    loss = loss_acc + loss_TV * weight_TV\n",
    "\n",
    "    loss.backward() #compute gradients\n",
    "\n",
    "    for p in model.parameters(): #we clip the gradient at norm 1\n",
    "      p.grad.data.clamp_(-1, 1)\n",
    "    \n",
    "    optimizer.step() #one SGD step\n",
    "    \n",
    "    loss_acc_meter.add(loss_acc.item())\n",
    "    loss_TV_meter.add(loss_TV.item())\n",
    "    loss_meter.add(loss.item())\n",
    "    labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "    #need to put the prediction back on the cpu and convert to numpy\n",
    "    cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "    \n",
    "  return cm, loss_meter.value()[0], loss_acc_meter.value()[0], loss_TV_meter.value()[0] \n",
    "\n",
    "def eval_TV(model, edges, weight_TV, args):\n",
    "  \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "  model.eval() #switch in eval mode\n",
    "  \n",
    "  #the loader function will take care of the batching\n",
    "  loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False)\n",
    "  #tqdm will provide some nice progress bars\n",
    "  loader = tqdm(loader, ncols=500)\n",
    "  \n",
    "  #will keep track of the loss\n",
    "  loss_meter = tnt.meter.AverageValueMeter()\n",
    "  loss_acc_meter = tnt.meter.AverageValueMeter()\n",
    "  loss_TV_meter = tnt.meter.AverageValueMeter()\n",
    "  cm = ConfusionMatrix(args.n_class, class_names = class_names)\n",
    "\n",
    "  for index, (tiles, gt) in enumerate(loader):\n",
    "     \n",
    "    pred = model(tiles) #compute the prediction\n",
    "\n",
    "    loss_acc = nn.functional.cross_entropy(pred.cpu(),gt, ignore_index=99)\n",
    "    loss_TV = TV(pred.cpu(), edges)\n",
    "\n",
    "    loss = loss_acc + loss_TV * weight_TV\n",
    "    \n",
    "    loss_acc_meter.add(loss_acc.item())\n",
    "    loss_TV_meter.add(loss_TV.item())\n",
    "    loss_meter.add(loss.item())\n",
    "    labeled = np.where(gt.view(-1)!=99)[0] #select gt with a label\n",
    "    #need to put the prediction back on the cpu and convert to numpy\n",
    "    cm.add_batch(gt.view(-1)[labeled], pred.argmax(1).view(-1)[labeled].cpu().detach().numpy())\n",
    "    \n",
    "  return cm, loss_meter.value()[0], loss_acc_meter.value()[0], loss_TV_meter.value()[0] \n",
    "  return cm, loss_meter.value()[0]\n",
    "\n",
    "def train_full_TV(args):\n",
    "  \"\"\"The full training loop\"\"\"\n",
    "  #initialize the model\n",
    "  \n",
    "  model = SegNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class)\n",
    "\n",
    "  edges = compute_adjacency_graph(256)\n",
    "\n",
    "  print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "  #define the optimizer\n",
    "  #adam optimizer is always a good guess for classification\n",
    "  optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "  \n",
    "  TESTCOLOR = '\\033[104m'\n",
    "  TRAINCOLOR = '\\033[100m'\n",
    "  NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "  for i_epoch in range(args.n_epoch):\n",
    "    #train one epoch\n",
    "    weight_TV = args.weight_TV_base * i_epoch / args.n_epoch\n",
    "    cm_train, loss_train, loss_acc_train, loss_TV_train = train_TV(model, optimizer, edges, weight_TV, args)\n",
    "    print(TRAINCOLOR)\n",
    "    print('Epoch %3d -> Overall Accuracy: %3.2f%% mIoU : %3.2f%% Acc Loss: %1.4f TV Loss: %1.4f Loss: %1.4f' % \\\n",
    "          (i_epoch, cm_train.overall_accuracy(), cm_train.class_IoU(), loss_acc_train, loss_TV_train, loss_train) + NORMALCOLOR)\n",
    "\n",
    "    if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "      #periodic testing\n",
    "      cm_test, loss_test, loss_acc_test, loss_TV_test = eval_TV(model, edges, weight_TV, args)\n",
    "      print(TESTCOLOR)\n",
    "      print('Epoch %3d -> Overall Accuracy: %3.2f%% mIoU : %3.2f%% Acc Loss: %1.4f TV Loss: %1.4f Loss: %1.4f' % \\\n",
    "          (i_epoch, cm_test.overall_accuracy(), cm_test.class_IoU(), loss_acc_test, loss_TV_test, loss_test) + NORMALCOLOR)\n",
    "      viewer(n_shown = 1, train = False, model = model, category = 'cigpe', use_mask = False)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZfN71CVaYJr"
   },
   "outputs": [],
   "source": [
    "#[12]\n",
    "args = mock.Mock() #stores the parameters\n",
    "args.n_epoch = 50\n",
    "args.n_epoch_test = int(5) #periodicity of evaluation on test set\n",
    "args.batch_size = 16\n",
    "args.n_class = 6\n",
    "args.n_channel = 4\n",
    "args.conv_width = [16,16,32,32,64,32]\n",
    "args.dconv_width = [32,16,32,16]\n",
    "args.cuda = 1\n",
    "args.lr = 5e-3\n",
    "args.weight_TV_base = 0.01\n",
    "\n",
    "trained_model = train_full_TV(args)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of segnet",
   "provenance": [
    {
     "file_id": "1xsDLMlPB7pqF56oWUSQaZpgvXy3OQgjo",
     "timestamp": 1587196814149
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
