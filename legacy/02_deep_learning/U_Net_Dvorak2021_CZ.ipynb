{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rady pro použití\n",
    "1. Změň dataset_path - cesta do složky s daty, tato složka by měla obsahovat podsložku s obrazovými daty (MHS, CIR, RGB nebo PAN), podsložku s referenčními daty (GT) a prázdné složky pro ukládání výsledků (results) a natrénovaných modelů (models)\n",
    "2. Změň use_mhs - číslo v této proměnné udává počet vstupních spektrálních pásem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles to be processed: \n",
      "225\n",
      "\n",
      "Total number of bands used: \n",
      "6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from time import time as time\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, jaccard_score\n",
    "from sklearn.model_selection import KFold\n",
    "import torchnet as tnt\n",
    "import functools\n",
    "import mock\n",
    "from tqdm import notebook as tqdm\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 12                                     # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize]  \n",
    "CMAP = matplotlib.colors.ListedColormap(['black', 'white', 'orange'])               # Color mapping \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision\n",
    "\n",
    "# PATHS TO TRAIN/TEST DATA\n",
    "dataset_path = 'e:\\\\datasets\\\\test_unet\\\\Krkonose2012\\\\overlap'\n",
    "num_of_tiles = len(os.listdir(os.path.join(dataset_path, 'GT')))\n",
    "print(f'Number of tiles to be processed: \\n{num_of_tiles}\\n')\n",
    "\n",
    "# USE CIR, RGB, PAN DATA\n",
    "use_cir = False\n",
    "use_rgb = False\n",
    "use_pan = False\n",
    "\n",
    "# USE multi/hyperspectral DATA (first value is a bool similar to use_rgb etc. and second value is the number of bands)\n",
    "use_mhs = (True, 6)\n",
    "\n",
    "print(f'Total number of bands used: \\n{use_cir*3 + use_rgb*3 + use_pan + use_mhs[0]*use_mhs[1]}')\n",
    "\n",
    "# MODEL NAME... USED AS FILENAME OF SAVED MODEL AND FOR APPROPRIATE RESULTS FOLDER\n",
    "model_name = 'U_Net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imagery(img_dir):\n",
    "    \"\"\"Reads the individual imagery patches and prepares them for \"\"\"\n",
    "    img_file_list = os.listdir(img_dir)\n",
    "    img_list = []\n",
    "        \n",
    "    for file in img_file_list:\n",
    "        img_patch = imageio.imread(os.path.join(img_dir, file)).astype(np.float32)\n",
    "        img_patch = img_patch[:,:,:].transpose([2,0,1])\n",
    "        img_patch = img_patch * 1/255\n",
    "            \n",
    "        img_list.append(img_patch)\n",
    "        del img_patch\n",
    "\n",
    "    img_features = np.stack(img_list, axis=0)\n",
    "    return img_features\n",
    "\n",
    "def read_patch(root_folder, cir, rgb, pan, mhs, gt=True):\n",
    "    ##########################################################\n",
    "    # READ IMAGES as FLOAT\n",
    "    \n",
    "    if cir:\n",
    "        cir_features = read_imagery(os.path.join(root_folder, 'CIR'))\n",
    "    if rgb:\n",
    "        rgb_features = read_imagery(os.path.join(root_folder, 'RGB'))\n",
    "    if mhs[0]:\n",
    "        mhs_features = read_imagery(os.path.join(root_folder, 'MHS'))\n",
    "\n",
    "    if pan:\n",
    "        pan_file_list = os.listdir(os.path.join(root_folder, 'PAN'))\n",
    "        pan_list = []\n",
    "        for file in pan_file_list:\n",
    "            pan_patch = imageio.imread(os.path.join(root_folder, 'PAN', file)).astype(np.float32)\n",
    "            pan_patch = pan_patch * 1/255\n",
    "            pan_patch = np.expand_dims(pan_patch, axis=0)\n",
    "            pan_list.append(pan_patch)\n",
    "            del pan_patch\n",
    "        pan_features = np.stack(pan_list, axis=0)\n",
    "\n",
    "\n",
    "    if cir and rgb:\n",
    "        features = np.concatenate([cir_features, rgb_features], axis=1)\n",
    "    elif cir:\n",
    "        features = cir_features\n",
    "    elif rgb:\n",
    "        features = rgb_features\n",
    "    elif pan:\n",
    "        features = pan_features\n",
    "    elif mhs:\n",
    "        features = mhs_features\n",
    "    else:\n",
    "        print('No valid data input.')\n",
    "    features = torch.from_numpy(features)\n",
    "    \n",
    "    \n",
    "    if gt:\n",
    "        gt_file_list = os.listdir(os.path.join(root_folder, 'GT'))\n",
    "        gt_list = []\n",
    "\n",
    "        for file in gt_file_list:\n",
    "            gt_patch = imageio.imread(os.path.join(root_folder, 'GT', file)).astype(np.int64)\n",
    "            # assigns 0 to classes 3 and above\n",
    "            # gt_patch[gt_patch > 2] = 0\n",
    "            \n",
    "            gt_list.append(gt_patch[:,:])\n",
    "            del gt_patch\n",
    "\n",
    "        ground_truth = np.stack(gt_list, axis=0)\n",
    "        ground_truth = torch.from_numpy(ground_truth)\n",
    "    \n",
    "    if gt:\n",
    "        return features, ground_truth\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of image data: \n",
      "torch.Size([225, 6, 256, 256])\n",
      "\n",
      "Size of reference data: \n",
      "torch.Size([225, 256, 256])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### putting the dataset into the TensorDataset wrapper\n",
    "data_features, data_labels = read_patch(dataset_path, use_cir, use_rgb, use_pan, use_mhs)\n",
    "\n",
    "print(f'Size of image data: \\n{data_features.shape}\\n')\n",
    "print(f'Size of reference data: \\n{data_labels.shape}\\n')\n",
    "\n",
    "dataset = tnt.dataset.TensorDataset(list([data_features, data_labels]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class labels: \n",
      "[0 1 2]\n",
      "\n",
      "Number of pixels in a class: \n",
      "[9834937 4148500  762163]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(data_labels, return_counts=True)\n",
    "print(f'Class labels: \\n{unique}\\n')\n",
    "print(f'Number of pixels in a class: \\n{counts}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net for semantic segmentation\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        n_channels, int, number of input channel\n",
    "        encoder_conv_width, int list, size of the feature maps of convs for the encoder\n",
    "        decoder_conv_width, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__() #necessary for all classes extending the module class\n",
    "    \n",
    "        self.maxpool=nn.MaxPool2d(2,2,return_indices=False) #maxpooling layer\n",
    "        self.dropout=nn.Dropout2d(p=0.5, inplace=True)\n",
    "    \n",
    "        #encoder\n",
    "        self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[0]),nn.ReLU())\n",
    "        self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[1]),nn.ReLU())\n",
    "        self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[2]),nn.ReLU())\n",
    "        self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[3]),nn.ReLU())\n",
    "        self.c5 = nn.Sequential(nn.Conv2d(encoder_conv_width[3],encoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[4]),nn.ReLU())\n",
    "        self.c6 = nn.Sequential(nn.Conv2d(encoder_conv_width[4],encoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[5]),nn.ReLU())\n",
    "        self.c7 = nn.Sequential(nn.Conv2d(encoder_conv_width[5],encoder_conv_width[6],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[6]),nn.ReLU())\n",
    "        self.c8 = nn.Sequential(nn.Conv2d(encoder_conv_width[6],encoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[7]),nn.ReLU())\n",
    "        self.c9 = nn.Sequential(nn.Conv2d(encoder_conv_width[7],encoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[8]),nn.ReLU())\n",
    "        self.c10 = nn.Sequential(nn.Conv2d(encoder_conv_width[8],encoder_conv_width[9],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(encoder_conv_width[9]),nn.ReLU())\n",
    "        #decoder\n",
    "        self.c11 = nn.ConvTranspose2d(encoder_conv_width[9], int(decoder_conv_width[0]/2),kernel_size=2, stride=2)\n",
    "        self.c12 = nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[1]),nn.ReLU())\n",
    "        self.c13 = nn.Sequential(nn.Conv2d(decoder_conv_width[1],decoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[2]),nn.ReLU())\n",
    "        self.c14 = nn.ConvTranspose2d(decoder_conv_width[2], int(decoder_conv_width[3]/2),kernel_size=2, stride=2)\n",
    "        self.c15 = nn.Sequential(nn.Conv2d(decoder_conv_width[3],decoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[4]),nn.ReLU())\n",
    "        self.c16 = nn.Sequential(nn.Conv2d(decoder_conv_width[4],decoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[5]),nn.ReLU())\n",
    "        self.c17 = nn.ConvTranspose2d(decoder_conv_width[5], int(decoder_conv_width[6]/2),kernel_size=2, stride=2)\n",
    "        self.c18 = nn.Sequential(nn.Conv2d(decoder_conv_width[6],decoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[7]),nn.ReLU())\n",
    "        self.c19 = nn.Sequential(nn.Conv2d(decoder_conv_width[7],decoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[8]),nn.ReLU())\n",
    "        self.c20 = nn.ConvTranspose2d(decoder_conv_width[8], int(decoder_conv_width[9]/2),kernel_size=2, stride=2)\n",
    "        self.c21 = nn.Sequential(nn.Conv2d(decoder_conv_width[9],decoder_conv_width[10],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[10]),nn.ReLU())\n",
    "        self.c22 = nn.Sequential(nn.Conv2d(decoder_conv_width[10],decoder_conv_width[11],3,padding=1, padding_mode='reflect'),nn.BatchNorm2d(decoder_conv_width[11]),nn.ReLU()) \n",
    "        \n",
    "        #final classifying layer\n",
    "        self.classifier=nn.Conv2d(decoder_conv_width[11],n_class,1,padding=0)\n",
    "\n",
    "        #weight initialization\n",
    "\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c12[0].apply(self.init_weights)\n",
    "        self.c13[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c15[0].apply(self.init_weights)\n",
    "        self.c16[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c18[0].apply(self.init_weights)\n",
    "        self.c19[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c21[0].apply(self.init_weights)\n",
    "        self.c22[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "    \n",
    "        if cuda: #put the model on the GPU memory\n",
    "            self.cuda()\n",
    "    \n",
    "    def init_weights(self,layer): #gaussian init for the conv layers\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        the function called to run inference\n",
    "        \"\"\"  \n",
    "        #encoder\n",
    "        #level 1\n",
    "        x1 = self.c2(self.c1(input))\n",
    "        x2 = self.maxpool(x1)\n",
    "        #level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        x4 = self.maxpool(x3)\n",
    "        #level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        x6 = self.maxpool(x5)\n",
    "        #Level 4\n",
    "        x7 = self.c8(self.c7(x6))\n",
    "        x8 = self.maxpool(x7)\n",
    "        #Level 5\n",
    "        x9 = self.c10(self.c9(x8))\n",
    "        #decoder\n",
    "        #Level 4\n",
    "        y8 = torch.cat((self.c11(x9),x7),1)\n",
    "        y7 = self.c13(self.c12(y8))\n",
    "        #Level 3\n",
    "        y6 = torch.cat((self.c14(y7),x5),1)\n",
    "        y5 = self.c16(self.c15(y6))\n",
    "        #level 2\n",
    "        y4 = torch.cat((self.c17(y5),x3),1)\n",
    "        y3 = self.c19(self.c18(y4))\n",
    "        #level 1       \n",
    "        y2 = torch.cat((self.c20(y3),x1),1)\n",
    "        y1 = self.c22(self.c21(y2))\n",
    "        #output         \n",
    "        out = self.classifier(self.dropout(y1))\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(obs, g_t):\n",
    "    \"\"\"the data augmentation function, introduces random noise and rotation\"\"\"\n",
    "    sigma, clip= 0.01, 0.03 \n",
    "    #Hint: use np.clip to clip and np.random.randn to generate gaussian noise\n",
    "    obs = obs + np.clip(sigma*np.random.randn(), -clip, clip).astype(np.float32).copy()\n",
    "\n",
    "    #random rotation 0 90 180 270 degree\n",
    "    n_turn = np.random.randint(4) #number of 90 degree truens, random int between 0 and 3\n",
    "    obs = np.rot90(obs, n_turn, axes=(2,3)).copy()\n",
    "    g_t = np.rot90(g_t, n_turn, axes=(1,2)).copy()\n",
    "\n",
    "    obs = torch.from_numpy(obs)\n",
    "    g_t = torch.from_numpy(g_t)\n",
    "    \n",
    "    return obs, g_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, args):\n",
    "    \"\"\"train for one epoch\"\"\"\n",
    "    model.train() #switch the model in training mode\n",
    "  \n",
    "    #the loader function will take care of the batching\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, sampler=args.train_subsampler)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    #will keep track of the loss\n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    for index, (tiles, gt) in enumerate(loader):\n",
    "    \n",
    "        optimizer.zero_grad() #put gradient to zero\n",
    "                \n",
    "        tiles, gt = augment(tiles, gt)\n",
    "    \n",
    "        pred = model(tiles.cuda()) #compute the prediction\n",
    "\n",
    "        loss = nn.functional.cross_entropy(pred.cpu(),gt, weight=torch.tensor(args.class_weights))\n",
    "\n",
    "        loss.backward() #compute gradients\n",
    "\n",
    "        for p in model.parameters(): #we clip the gradient at norm 1\n",
    "            p.grad.data.clamp_(-1, 1) #this helps learning faster\n",
    "    \n",
    "        optimizer.step() #one SGD step\n",
    "    \n",
    "        loss_meter.add(loss.item())\n",
    "        \n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "def eval(model, args):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "  \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, sampler=args.test_subsampler)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "  \n",
    "    loss_meter = tnt.meter.AverageValueMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda())\n",
    "            loss = nn.functional.cross_entropy(pred.cpu(),gt)\n",
    "            loss_meter.add(loss.item())\n",
    "\n",
    "    return loss_meter.value()[0]\n",
    "\n",
    "\n",
    "def train_full(args):\n",
    "    \"\"\"The full training loop\"\"\"\n",
    "\n",
    "    #initialize the model\n",
    "    model = UNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class, args.cuda)\n",
    "\n",
    "    print('Total number of parameters: {}'.format(sum([p.numel() for p in model.parameters()])))\n",
    "  \n",
    "    #define the optimizer\n",
    "    #adam optimizer is always a good guess for classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[60,80,95], gamma=0.3)\n",
    "  \n",
    "    TESTCOLOR = '\\033[104m'\n",
    "    NORMALCOLOR = '\\033[0m'\n",
    "  \n",
    "    train_loss = np.empty(args.n_epoch)\n",
    "    test_loss = np.empty(args.n_epoch//args.n_epoch_test)\n",
    "    test_i = 0\n",
    "\n",
    "    for i_epoch in range(args.n_epoch):\n",
    "        #train one epoch\n",
    "        print('Epoch ' + str(i_epoch))\n",
    "        loss_train = train(model, optimizer, args)\n",
    "        scheduler.step()\n",
    "        train_loss[i_epoch] = loss_train\n",
    "\n",
    "        if (i_epoch == args.n_epoch - 1) or (args.n_epoch_test != 0 and i_epoch % args.n_epoch_test == 0 and i_epoch > 0):\n",
    "            #periodic testing\n",
    "            print(TESTCOLOR)\n",
    "            print('Evaluation')\n",
    "            loss_test = eval(model, args)\n",
    "            test_loss[test_i] = loss_test\n",
    "            test_i += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(1,1,1,ylim=(0,2), xlabel='Epoch #', ylabel='Loss')\n",
    "    plt.plot(range(args.n_epoch), train_loss)\n",
    "    plt.plot(range(args.n_epoch_test-1, args.n_epoch, args.n_epoch_test), test_loss)\n",
    "    plt.show()\n",
    "    print(train_loss)\n",
    "    print(test_loss)\n",
    "    args.loss_test = loss_test\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rady pro použití 2\n",
    "1. Změň args.n_class - počet klasifikačních tříd\n",
    "2. Změň args.class weights, tak aby jejich součet zůstal 1 (váha tříd při tréninku, méně zastoupené třídy lze klasifikovat lépe díky vyšší hodnotě této proměnné)\n",
    "3. Experimentuj s různými hodnotami args.n_epoch (počet trénovacích epoch), args.lr (learning rate, rychlost se kterou se sit uci) a args.batch_size (počet snímků v minibatchi pro trénink, výrazně omezeno pamětí grafické karty)\n",
    "4. Hodnotami na konci proměnných args.conv_width a args.dconv_width (druhé parametry funkce np.divide()) lze změnit \"velikost sítě\" - počet konvolučních filtrů v každé vrstvě sítě... 1 znamená U-Net v originální podobě (Ronneberger et al. 2015), doporučuji začít s hodnotou 2 (poloviční síť oporoti původní) a případně snížit hodnotu kdyby se ukázalo, že se model nezvládá naučit prostorové vztahy ve snímku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args = mock.Mock() #stores the parameters\n",
    "args.n_epoch = 50\n",
    "args.n_epoch_test = int(5) #periodicity of evaluation on test set\n",
    "args.batch_size = 1\n",
    "args.n_class = 3\n",
    "args.n_channel = use_cir*3 + use_rgb*3 + use_pan + use_mhs[0]*use_mhs[1]\n",
    "args.conv_width =  np.divide([64,64,128,128,256,256,512,512,1024,1024],        4).astype(np.int)\n",
    "args.dconv_width = np.divide([1024,512,512,512,256,256,256,128,128,128,64,64], 4).astype(np.int)\n",
    "args.class_weights = [0.1, 0.1, 0.8]\n",
    "args.cuda = True\n",
    "args.lr = 5e-4\n",
    "args.crossval_nfolds = 3\n",
    "args.model_save_folder = os.path.join(dataset_path, 'models')\n",
    "kfold = KFold(n_splits = args.crossval_nfolds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(75,)\n",
      "Total number of parameters: 1944515\n",
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf1da5938024478a4b80b71a223bf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7bfadc7ee947ef9fbf45caa664efc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bea18755c4c427095e90e57e082b1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1e0fe915ecf7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrained_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mmodel_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5f6755e13214>\u001b[0m in \u001b[0;36mtrain_full\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m#train one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi_epoch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-5f6755e13214>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, args)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#put gradient to zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maugment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtiles\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\ML_in_geosciences\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    215\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model training itself, using crossvalidation\n",
    "model_results = {}\n",
    "for fold, (train_ids, test_ids) in enumerate(kfold.split(dataset)):\n",
    "    args.train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "    args.test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
    "    \n",
    "    print(train_ids.shape)\n",
    "    print(test_ids.shape)\n",
    "    a = time()\n",
    "    trained_model = train_full(args)\n",
    "    model_results[fold] = args.loss_test\n",
    "    \n",
    "    print('Saving the model to:')\n",
    "    state_dict_path = os.path.join(args.model_save_folder, f'{model_name}_fold_{str(fold)}.pt')\n",
    "    print(state_dict_path)\n",
    "    torch.save(trained_model.state_dict(), state_dict_path)\n",
    "    print(f'Training finished in {str(time()-a)}s')\n",
    "\n",
    "print('\\n')\n",
    "print(f'Resulting loss for individual folds: \\n{model_results}')\n",
    "print(f'Mean loss across all folds: \\n{np.mean(model_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a trained model\n",
    "Change state_dict_path to the trained model you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the state_dictionary\n",
    "state_dict_path = 'E:\\\\datasets\\\\test_unet\\\\Krkonose2012\\\\overlap\\\\models\\\\fold_0.pt'\n",
    "\n",
    "# Parameters for model definition\n",
    "args = mock.Mock() #stores the parameters\n",
    "\n",
    "args.n_class = 3\n",
    "args.n_channel = 6 # 6 if use_cir and use_rgb else 3\n",
    "args.conv_width =  np.divide([64,64,128,128,256,256,512,512,1024,1024],        4).astype(np.int)\n",
    "args.dconv_width = np.divide([1024,512,512,512,256,256,256,128,128,128,64,64], 4).astype(np.int)\n",
    "args.cuda = True\n",
    "\n",
    "# Load a trained model state_dictionary\n",
    "model = UNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class, args.cuda)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing accuracy metrics\n",
    "Computes precision, recall and f1-score for each class as well as overall accuracy and mean f1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(model, args):\n",
    "    \"\"\"eval on test/validation set\"\"\"\n",
    "  \n",
    "    model.eval() #switch in eval mode\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, drop_last=False)\n",
    "    loader = tqdm.tqdm(loader, ncols=500)\n",
    "    \n",
    "    classified = np.empty_like(y_t.detach().numpy())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, (tiles, gt) in enumerate(loader):\n",
    "            pred = model(tiles.cuda()).cpu().detach().numpy()\n",
    "            classified[index, :, :] = pred.squeeze().argmax(0)\n",
    "\n",
    "    return classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pixels belonging to each class in the reference dataset\n",
    "\n",
    "y_t = data_labels\n",
    "y_t_flat = y_t.detach().numpy().flatten()\n",
    "\n",
    "unique, counts = np.unique(y_t_flat, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Number of pixels belonging to each class in the classified dataset\n",
    "\n",
    "a = time()\n",
    "Y_t = classify(model, args)\n",
    "b = time()\n",
    "print('Inferrence finished in ' + str(b-a) + ' s')\n",
    "\n",
    "Y_t_flat = Y_t.flatten()\n",
    "\n",
    "unique, counts = np.unique(Y_t_flat, return_counts=True)\n",
    "print(unique)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy metrics\n",
    "\n",
    "precisions, recalls, f1_scores, supports = precision_recall_fscore_support(y_t_flat, Y_t_flat)\n",
    "overall_accuracy = accuracy_score(y_t_flat, Y_t_flat)\n",
    "mean_f1_score = sum(f1_scores)/len(f1_scores)\n",
    "\n",
    "print('precisions [%]:      ', precisions*100)\n",
    "print('recalls    [%]:      ', recalls*100)\n",
    "print('f1_scores  [%]:      ', f1_scores*100)\n",
    "print('')\n",
    "print('overall accuracy: {:.2%}'.format(overall_accuracy))\n",
    "print('mean f1 score:    {:.2%}'.format(mean_f1_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results\n",
    "Results are not georeferenced – use Georeference_results_gdal.ipynb for georeferencing and combining into a single raster\n",
    "Change source_path to the origina root directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = 'E:\\\\datasets\\\\test_unet\\\\Krkonose2012\\\\overlap'\n",
    "results_path = os.path.join(source_path, 'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images to classify\n",
    "in_features = read_patch(source_path, use_cir, use_rgb, use_pan, use_mhs, gt=False)\n",
    "print(in_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create files for classified tiles\n",
    "if use_rgb:\n",
    "    copy_tree(os.path.join(source_path, 'RGB'), results_path, update=1)\n",
    "elif use_cir:\n",
    "    copy_tree(os.path.join(source_path, 'CIR'), results_path, update=1)\n",
    "elif use_pan:\n",
    "    copy_tree(os.path.join(source_path, 'PAN'), results_path, update=1)\n",
    "elif use_mhs[0]:\n",
    "    copy_tree(os.path.join(source_path, 'MHS'), results_path, update=1)\n",
    "else:\n",
    "    print('no input files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_export(model_b, in_features_b, results_path_b):\n",
    "    for i, patch in enumerate(os.listdir(results_path_b)):\n",
    "        in_patch = in_features_b[i,:,:,:]\n",
    "        pred = model_b(in_patch[None,:,:,:].cuda()).cpu().detach().numpy()\n",
    "        pred = pred[0,:,:,:].argmax(0).squeeze()\n",
    "\n",
    "        imageio.imwrite(results_path_b + patch, pred.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_and_export(model, in_features, results_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
