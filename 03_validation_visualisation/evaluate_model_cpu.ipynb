{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(512*512).reshape(512,512)\n",
    "print(x[128:384,128:384].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time as time\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torchnet as tnt\n",
    "import functools\n",
    "\n",
    "import mock\n",
    "from tqdm import notebook as tqdm\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "\n",
    "# GLOBAL SETTINGS\n",
    "PlotSize = 12                                     # Size of plots\n",
    "matplotlib.rcParams['figure.figsize'] = [PlotSize*2, PlotSize]  \n",
    "CMAP = matplotlib.colors.ListedColormap(['black', 'white', 'orange'])               # Color mapping \n",
    "np.set_printoptions(precision=2, suppress=True)  # Array print precision\n",
    "\n",
    "# CLASS AND FEATURE DESCRIPTION\n",
    "class_names = ['BACKGRD','PINUS','PICEA']\n",
    "\n",
    "# PATHS TO TRAIN/TEST DATA\n",
    "data_path = '../data/66_33/'\n",
    "training_set_path = data_path + 'train/'         # Relative path to training patch root folder\n",
    "test_set_path =     data_path + '/test/'          # Relative path to test patch root folder\n",
    "\n",
    "num_of_training_tiles = len(os.listdir(training_set_path + 'GT/'))\n",
    "num_of_test_tiles = len(os.listdir(test_set_path + 'GT/'))\n",
    "\n",
    "# USE CIR OR RGB DATA\n",
    "use_cir = True\n",
    "use_rgb = True\n",
    "\n",
    "# MODEL NAME... USED AS FILENAME OF SAVED MODEL AND FOR APPROPRIATE RESULTS FOLDER\n",
    "model_name = 'U_Net_66_33'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_patch(root_folder, cir, rgb, gt=True):\n",
    "    ##########################################################\n",
    "    # READ IMAGES as FLOAT\n",
    "    \n",
    "    if cir:\n",
    "        cir_file_list = os.listdir(root_folder + 'CIR/')\n",
    "        cir_list = []\n",
    "        \n",
    "        for file in cir_file_list:\n",
    "            cir_patch = imageio.imread(root_folder + 'CIR/' + file).astype(np.float32)\n",
    "            cir_patch = cir_patch[:,:,:].transpose([2,0,1])\n",
    "            cir_patch = cir_patch * 1/255\n",
    "            \n",
    "            cir_list.append(cir_patch)\n",
    "            del cir_patch\n",
    "\n",
    "        cir_features = np.stack(cir_list, axis=0)    \n",
    "    \n",
    "    if rgb:\n",
    "        rgb_file_list = os.listdir(root_folder + 'RGB/')\n",
    "        rgb_list = []\n",
    "        \n",
    "        for file in rgb_file_list:\n",
    "            rgb_patch = imageio.imread(root_folder + 'RGB/' + file).astype(np.float32)\n",
    "            rgb_patch = rgb_patch[:,:,:].transpose([2,0,1])\n",
    "            rgb_patch = rgb_patch * 1/255\n",
    "            \n",
    "            rgb_list.append(rgb_patch)\n",
    "            \n",
    "            del rgb_patch\n",
    "        \n",
    "        rgb_features = np.stack(rgb_list, axis=0)\n",
    "\n",
    "    if cir and rgb:\n",
    "        features = np.concatenate([cir_features, rgb_features], axis=1)\n",
    "    elif cir:\n",
    "        features = cir_features\n",
    "    elif rgb:\n",
    "        features = rgb_features\n",
    "    else:\n",
    "        print('No valid data input.')\n",
    "    features = torch.from_numpy(features)\n",
    "    \n",
    "    \n",
    "    if gt:\n",
    "        gt_file_list = os.listdir(root_folder + 'GT/')\n",
    "        gt_list = []\n",
    "\n",
    "        for file in gt_file_list:\n",
    "            gt_patch = imageio.imread(root_folder + 'GT/' + file).astype(np.int64)\n",
    " \n",
    "            gt_list.append(gt_patch[:,:])\n",
    "            del gt_patch\n",
    "\n",
    "        ground_truth = np.stack(gt_list, axis=0)\n",
    "        ground_truth = torch.from_numpy(ground_truth)\n",
    "    \n",
    "    if gt:\n",
    "        return features, ground_truth\n",
    "    else:\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net network for semantic segmentation\n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, n_channels, encoder_conv_width, decoder_conv_width, n_class, cuda):\n",
    "        \"\"\"\n",
    "        initialization function\n",
    "        n_channels, int, number of input channel\n",
    "        encoder_conv_width, int list, size of the feature maps of convs for the encoder\n",
    "        decoder_conv_width, int list, size of the feature maps of convs for the decoder\n",
    "        n_class = int,  the number of classes\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__() #necessary for all classes extending the module class\n",
    "    \n",
    "        self.maxpool=nn.MaxPool2d(2,2,return_indices=False) #maxpooling layer\n",
    "    \n",
    "        #encoder\n",
    "        self.c1 = nn.Sequential(nn.Conv2d(n_channels,encoder_conv_width[0],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c2 = nn.Sequential(nn.Conv2d(encoder_conv_width[0],encoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c3 = nn.Sequential(nn.Conv2d(encoder_conv_width[1],encoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c4 = nn.Sequential(nn.Conv2d(encoder_conv_width[2],encoder_conv_width[3],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c5 = nn.Sequential(nn.Conv2d(encoder_conv_width[3],encoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c6 = nn.Sequential(nn.Conv2d(encoder_conv_width[4],encoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c7 = nn.Sequential(nn.Conv2d(encoder_conv_width[5],encoder_conv_width[6],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c8 = nn.Sequential(nn.Conv2d(encoder_conv_width[6],encoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c9 = nn.Sequential(nn.Conv2d(encoder_conv_width[7],encoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c10 = nn.Sequential(nn.Conv2d(encoder_conv_width[8],encoder_conv_width[9],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        #decoder\n",
    "        self.c11 = nn.ConvTranspose2d(encoder_conv_width[9], int(decoder_conv_width[0]/2),kernel_size=2, stride=2)\n",
    "        self.c12 = nn.Sequential(nn.Conv2d(decoder_conv_width[0],decoder_conv_width[1],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c13 = nn.Sequential(nn.Conv2d(decoder_conv_width[1],decoder_conv_width[2],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c14 = nn.ConvTranspose2d(decoder_conv_width[2], int(decoder_conv_width[3]/2),kernel_size=2, stride=2)\n",
    "        self.c15 = nn.Sequential(nn.Conv2d(decoder_conv_width[3],decoder_conv_width[4],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c16 = nn.Sequential(nn.Conv2d(decoder_conv_width[4],decoder_conv_width[5],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c17 = nn.ConvTranspose2d(decoder_conv_width[5], int(decoder_conv_width[6]/2),kernel_size=2, stride=2)\n",
    "        self.c18 = nn.Sequential(nn.Conv2d(decoder_conv_width[6],decoder_conv_width[7],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c19 = nn.Sequential(nn.Conv2d(decoder_conv_width[7],decoder_conv_width[8],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c20 = nn.ConvTranspose2d(decoder_conv_width[8], int(decoder_conv_width[9]/2),kernel_size=2, stride=2)\n",
    "        self.c21 = nn.Sequential(nn.Conv2d(decoder_conv_width[9],decoder_conv_width[10],3,padding=1, padding_mode='reflect'),nn.ReLU(True))\n",
    "        self.c22 = nn.Sequential(nn.Conv2d(decoder_conv_width[10],decoder_conv_width[11],3,padding=1, padding_mode='reflect'),nn.ReLU(True)) \n",
    "        \n",
    "        #final classifying layer\n",
    "        self.classifier=nn.Conv2d(decoder_conv_width[11],n_class,1,padding=0)\n",
    "\n",
    "        #weight initialization\n",
    "\n",
    "        self.c1[0].apply(self.init_weights)\n",
    "        self.c2[0].apply(self.init_weights)\n",
    "        self.c3[0].apply(self.init_weights)\n",
    "        self.c4[0].apply(self.init_weights)\n",
    "        self.c5[0].apply(self.init_weights)\n",
    "        self.c6[0].apply(self.init_weights)\n",
    "        self.c7[0].apply(self.init_weights)\n",
    "        self.c8[0].apply(self.init_weights)\n",
    "        self.c9[0].apply(self.init_weights)\n",
    "        self.c10[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c12[0].apply(self.init_weights)\n",
    "        self.c13[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c15[0].apply(self.init_weights)\n",
    "        self.c16[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c18[0].apply(self.init_weights)\n",
    "        self.c19[0].apply(self.init_weights)\n",
    "        \n",
    "        self.c21[0].apply(self.init_weights)\n",
    "        self.c22[0].apply(self.init_weights)\n",
    "        self.classifier.apply(self.init_weights)\n",
    "    \n",
    "        if cuda: #put the model on the GPU memory\n",
    "            self.cuda()\n",
    "    \n",
    "    def init_weights(self,layer): #gaussian init for the conv layers\n",
    "        nn.init.kaiming_normal_(layer.weight, mode='fan_out', nonlinearity='relu')\n",
    "    \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        the function called to run inference\n",
    "        \"\"\"  \n",
    "        #encoder\n",
    "        #level 1\n",
    "        x1 = self.c2(self.c1(input))\n",
    "        x2 = self.maxpool(x1)\n",
    "        #level 2\n",
    "        x3 = self.c4(self.c3(x2))\n",
    "        del x2\n",
    "        x4 = self.maxpool(x3)\n",
    "        #level 3\n",
    "        x5 = self.c6(self.c5(x4))\n",
    "        del x4\n",
    "        x6 = self.maxpool(x5)\n",
    "        #Level 4\n",
    "        x7 = self.c8(self.c7(x6))\n",
    "        del x6\n",
    "        x8 = self.maxpool(x7)\n",
    "        #Level 5\n",
    "        x9 = self.c10(self.c9(x8))\n",
    "        del x8\n",
    "        #decoder\n",
    "        #Level 4\n",
    "        y8 = torch.cat((self.c11(x9),x7),1)\n",
    "        del x7,x9\n",
    "        y7 = self.c13(self.c12(y8))\n",
    "        del y8\n",
    "        #Level 3\n",
    "        y6 = torch.cat((self.c14(y7),x5),1)\n",
    "        del x5,y7\n",
    "        y5 = self.c16(self.c15(y6))\n",
    "        del y6\n",
    "        #level 2\n",
    "        y4 = torch.cat((self.c17(y5),x3),1)\n",
    "        del x3, y5\n",
    "        y3 = self.c19(self.c18(y4))\n",
    "        del y4\n",
    "        #level 1       \n",
    "        y2 = torch.cat((self.c20(y3),x1),1)\n",
    "        del x1, y3\n",
    "        y1 = self.c22(self.c21(y2))\n",
    "        del y2\n",
    "        #output         \n",
    "        out = self.classifier(y1)\n",
    "    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the state_dictionary\n",
    "state_dict_path = 'trained_models/U_Net_66_33.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for model definition\n",
    "args = mock.Mock() #stores the parameters\n",
    "\n",
    "args.n_class = len(class_names)\n",
    "args.n_channel = 6 if use_cir and use_rgb else 3\n",
    "args.conv_width = [64,64,128,128,256,256,512,512,1024,1024]\n",
    "args.dconv_width = [1024,512,512,512,256,256,256,128,128,128,64,64]\n",
    "args.cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a trained model state_dictionary\n",
    "model = UNet(args.n_channel, args.conv_width, args.dconv_width, args.n_class, args.cuda)\n",
    "model.load_state_dict(torch.load(state_dict_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_export(model_b, in_features_b, results_path_b, gpu):\n",
    "    i = 0\n",
    "    for patch in os.listdir(results_path_b):\n",
    "        in_patch = in_features_b[i,:,:,:]\n",
    "        if gpu:\n",
    "            pred = model_b(in_patch[None,:,:,:].cuda()).cpu().detach().numpy()\n",
    "        else:\n",
    "            pred = model_b(in_patch[None,:,:,:]).detach().numpy()\n",
    "        pred = pred[0,:,:,:].argmax(0).squeeze()\n",
    "\n",
    "        imageio.imwrite(results_path_b + patch, pred.astype(np.uint8))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_paths = ['../data/2012/western/transects/']\n",
    "results_paths = ['../data/2012/western/transects/results/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for source_path in source_paths:\n",
    "    a = time()\n",
    "    in_features = read_patch(source_path, use_cir, use_rgb, gt=False)\n",
    "    b = time()\n",
    "    print('Dataset loaded in ' + str(b-a) + ' s')\n",
    "    \n",
    "    if use_rgb:\n",
    "        copy_tree(source_path + 'RGB/', results_paths[i], update=1)\n",
    "    elif use_cir:\n",
    "        copy_tree(source_path + 'CIR/', results_paths[i], update=1)\n",
    "    else:\n",
    "        print('no input files')\n",
    "    c = time()\n",
    "    classify_and_export(model, in_features, results_paths[i], args.cuda)\n",
    "    d = time()\n",
    "    print('Classified and saved in ' + str(d-c) + ' s')\n",
    "    del in_features\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
